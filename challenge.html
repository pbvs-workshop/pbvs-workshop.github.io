<!DOCTYPE html>
<html>
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <link rel="icon" href="images/logo_o_png.ico"><title>Welcome to PBVS '26</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="./css/bootstrap.min.css">
    <script src="./css/jquery.min.js.descarga"></script>
    <script src="./css/bootstrap.min.js.descarga"></script>
	<script src="./js/scrolling-nav.js"></script>
    <link href="./css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="./css/style.css" rel="stylesheet" type="text/css">
	<link href="./css/scrolling-nav.css" rel="stylesheet">
<style>
    #content li {
       margin-left:-20px;  
    }   
</style>
</head>

<body>

<div class="navbar navbar-default navbar-fixed-top" id="navi_bar">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar-ex-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="history.html" style="color:rgb(111,84,153);"><span><strong>22 YEARS of PBVS</strong></span></a>
        </div>
        <div class="collapse navbar-collapse" id="navbar-ex-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li ><a href="index.html"><strong>Home</strong></a></li>
                <li><a href="call_for_paper.html"><strong>Call for Paper</strong></a></li>
                <li  class="active"><a href="challenge.html"><strong>Challenges</strong></a></li>
                <li><a href="submission.html"><strong>Submission</strong></a></li>
                <li><a href="organization.html"><strong>Organization</strong></a></li>
                <li><a href="program.html"><strong>Program</strong></a></li>
                <li><a href="keynotes.html"><strong>Keynotes</strong></a></li>
                <li><a href="datasets.html"><strong>Datasets</strong></a></li>
                <li><a href="awards.html"><strong>Awards</strong></a></li>
                <li><a href="history.html"><strong>History</strong></a></li>
            </ul>
        </div>
    </div>
</div>

<br><br>
<div class="section" id="announcement">
    <div class="container">
        <div class="row">
<p align="center"><strong>Announcement: this year, <u>SEVEN</u> challenges are being organized in the framework of PBVS 2026 workshop.</strong>
</p>
</div>
</div>

<div class="section" id="challenge_1">
    <div class="container">
        <div class="row">
            <div class="panel panel-primary">
                <div class="panel-heading">
                    <h3 class="panel-title text-center">7th Thermal Image Super-Resolution Challenge (TISR)</h3>
                </div>
                <div class="panel-body">
                    <div class="row">
                        <div class="col-md-12">
                            <p align="justify">The seventh Thermal Image Super-Resolution is a new edition of last year's challenge, which considers a cross-spectral dataset captured with visible (Basler camera) and thermal (TAU2 camera) sensors. It consists of two tracks. </p>
							<p align="justify"><i>Track 1</i> features a single evaluation task that requires participants to generate an x8 super-resolution thermal image from the given low-resolution thermal images. The challenge uses a noiseless set of images, bicubic down-sampled by a factor of 8, as input.</p>
							<P align="justify"><i>Track 2</i> consists of two evaluation tasks using the above-mentioned dataset. The <i>first evaluation</i> involves generating an x8 super-resolution thermal image, while the <i>second evaluation</i> requires participants to generate an x16 super-resolution thermal image. In both cases, the provided high-resolution visible image should be used as guidance for enhancing the low-resolution thermal image. The architectures proposed in this track must incorporate visible images as guidance. </P>
							<P align="justify">For further details and access to the dataset, please refer to the CodaLab page: </P> 
							<P><a href="https://www.codabench.org/competitions/12339/" class=content target="_blank"><strong>TRACK 1</strong></a></P>
							<P><a href="https://www.codabench.org/competitions/12145/" class=content target="_blank"><strong>TRACK 2</strong></a></P>
						</div>
                   </div>    
            	</div>
            </div>
        </div>
    </div>
</div>
<div class="section" id="challenge_1">
    <div class="container">
        <div class="row">
            <div class="panel panel-primary" id="challenge_2">
                <div class="panel-heading">
                    <h3 class="panel-title text-center">5th Multi-modal Aerial View Imagery Challenge: Classification (MAVIC-C)</h3>
                </div>
                <div class="panel-body">
                    <p align="justify">Electro-optical (EO) sensors that capture images in the visible spectrum such as RGB and grayscale images, have been most prevalent in the computer vision research area. However, other sensors such as synthetic aperture radar (SAR) can reproduce images from radar signals that in some cases could complement EO sensors when such sensors fail to capture significant information (i.e. weather conditions, no visible light, etc.).</p>
                    <p align="justify">An ideal automated target recognition system would be based on multi-sensor information to compensate for the shortcomings of either of the sensor-based platforms individually. However, it is currently unclear if/how using EO and SAR data together can improve the performance of automatic target recognition (ATR) systems. Thus, the motivation for this challenge is to understand if and how data from one modality can improve the learning process for the other modality and vice versa. Ideas from domain adaptation, transfer learning or fusion are welcomed to solve this problem.</p>
                    <p align="justify">In addition to target recognition, this challenge focuses on accuracy and out-of-distribution detection. A robust target recognition system would not only provide a labeled target but also a confidence score for the target. A low score would correspond to an out-of-distribution sample.</p>
                    <p align="justify">More details and dataset in CodaLab page:</P> 
                        <!-- <P><a href="https://codalab.lisn.upsaclay.fr/competitions/21243" class=content target="_blank"><strong>LINK</strong></a></P> -->
                         <p>Codabench link TBA</p>
                </div>
            </div>
        </div>
    </div>
</div>
<div class="section" id="challenge_1">
    <div class="container">
        <div class="row">            
            <div class="panel panel-primary" id="challenge_3">
                <div class="panel-heading">
                    <h3 class="panel-title text-center">4th Multi-modal Aerial View Imagery Challenge: Translation (MAVIC-T)</h3>
                </div>
                <div class="panel-body">
                    <p align="justify">We iterate on the sensor translation challenge introduced at PBVS 2023. Sensor translation algorithms allow for dataset augmentation and enable the fusion of information from multiple sensors. Multi-modal sensor translation and data generation has wide ranging applications. We introduce a custom, multi-modal paired image dataset consisting of Electro-optical (EO) and Synthetic Aperture Radar (SAR) paired images, and RGB-IR paired images. The motivation for this challenge is to facilitate state of the art techniques in high-fidelity, conditioned data generation. This competition challenges participants to design general methods to translate aligned images from multiple modalities.</p>
                    <p align="justify">
                    More details and dataset in CodaLab page:
                    </p>
                    <!-- <p><a href="https://codalab.lisn.upsaclay.fr/competitions/21244" class="content" target="_blank"><strong>LINK</strong></a></p> -->
                    <p>Codabench link TBA</p>
                    </p>
                </div>
            </div>
        </div>
    </div>
</div>

<div class="section" id="challenge_1">
    <div class="container">
        <div class="row">            
            <div class="panel panel-primary" id="challenge_4">
                <div class="panel-heading">
                    <h3 class="panel-title text-center">2nd Thermal Pedestrian Multiple Object Tracking Challenge (TP-MOT)</h3>
                </div>
                <div class="panel-body">
                    <p align="justify">The Thermal MOT Challenge is dedicated to advancing object tracking research in thermal imaging, a critical modality for scenarios where visible-light sensors may fail, such as low-light conditions, nighttime, and adverse weather. Unlike RGB-based datasets, which dominate MOT research, thermal imaging provides unique advantages by capturing long-wavelength infrared (LWIR) data, making it an essential tool for robust tracking in challenging environments.</p>
                    <p align="justify">This challenge introduces the Thermal MOT Dataset, the first large-scale thermal imaging dataset annotated specifically for multiple object tracking. The dataset comprises 30 sequences (9000 frames) collected at five urban intersections using a FLIR ADK thermal sensor. These sequences include diverse scenes and object types in public spaces, providing a comprehensive benchmark for thermal MOT research.</p>
                    <p align="justify">Participants are invited to develop innovative algorithms that leverage thermal data to improve tracking accuracy and robustness. The challenge emphasizes single-modality tracking in the thermal domain and encourages approaches that address the unique characteristics of thermal imagery, such as noise, resolution, and contrast.</p>
                    <p align="justify">Dataset download link: <a href="https://uottawa-my.sharepoint.com/personal/welah096_uottawa_ca/_layouts/15/guestaccess.aspx?share=EcbEBO2k3ZdDttBxcH_1CaEBPJJA42_aF7o6WQxhcCWvkg&e=zaCT9a">Sharepoint</a></p>
                    <p align="justify">Rules</p>
                    <ul>
                        <li>Solutions must adhere to the tracking-by-detection paradigm in a two-stage approach. An object detection model must be used to predict boxes, and a separate model/algorithm can be used to associate boxes into tracks.</li>
                        <li>For a fair comparison of MOT algorithms, the detectors used can be either YOLOV8s or YOLOV11s</li>
                        <li>Ground truth annotations for the training set are provided. Models must not be trained on images from the validation set.</li>
                        <li>Same tracking paramters should be used for all 6 test sequences. No unique parameters must be changed based on any particular sequence.</li>
                        <li><strong>To be considered in the competition, submissions must introduce an algorithmic or model improvement on existing solutions. Submissions that rely simply on hyper-parameter tuning or brute force tuning will not be considered.</strong></li>
                    </ul>

                    <p align="justify">Codabench link TBA.</p>
                </div>
            </div>
        </div>
    </div>
</div>

<div class="section" id="challenge_1">
    <div class="container">
        <div class="row">            
            <div class="panel panel-primary" id="challenge_5">
                <div class="panel-heading">
                    <h3 class="panel-title text-center">1st Hyperspectral Image Super-Resolution Challenge (HISR)</h3>
                </div>
                <div class="panel-body">
                    <p align="justify">The first edition of this challenge introduces hyperspectral data with real LR–HR pairs. Each scene is captured twice:</p>
                    <ol>
                        <li>low-resolution hyperspectral cube via a low-magnification lens</li>
                        <li>high-resolution hyperspectral cube via a high-magnification lens of the same field of view</li>
                    </ol>
                    <p> Task: x4 Real Super-Resolution</p>
                    <p>Participants must generate a x4 HR hyperspectral image from the given real LR input. Unlike synthetic bicubic setups, the LR data reflect true optical blur, resolution loss, and sensor noise. Evaluation considers a combination of spatial and spectral fidelity metrics.</p>
                    <p>Dataset access and submission details are available on the CodaBench page (link TBA)</p>
                    </p>
                </div>
            </div>
        </div>
    </div>
</div>

<div class="section" id="challenge_1">
    <div class="container">
        <div class="row">            
            <div class="panel panel-primary" id="challenge_6">
                <div class="panel-heading">
                    <h3 class="panel-title text-center">1st Mars Landslide Segmentation Challenge (MARS-LS)</h3>
                </div>
                <div class="panel-body">
                    <p align="justify">Detecting landslides on Mars is fundamentally more challenging than on Earth due to the absence of vegetation and the highly uniform, desert-like surface. Electro-optical imagery alone often lacks sufficient contrast to distinguish landslide features from surrounding terrain, as Martian landslides must be identified primarily through subtle morphological cues such as scarps, hummocky deposits, and runout patterns. While multi-modal data, such as RGB or grayscale imagery, digital elevation models, slope, and thermal inertia, can provide complementary information, each modality differs in spatial resolution, noise, and physical meaning, making direct integration difficult.</p>
                    <p align="justify"> An ideal Maritain landslide detection system would effectively leverage multi-modal data to overcome the limitations of individual modalities and improve robustness across diverse terrains. However, it remains unclear how best to fuse these heterogeneous data sources to enhance learning, generalization, and out-of-distribution performance, especially under limited labeled data. This challenge motivates the exploration of multi-modal fusion, attention-based models, and transfer or domain adaptation techniques to develop robust deep networks that achieve accurate landslide segmentation while maintaining strong generalization to unseen Martian regions.</p>
                    <p>Participants must generate a x4 HR hyperspectral image from the given real LR input. Unlike synthetic bicubic setups, the LR data reflect true optical blur, resolution loss, and sensor noise. Evaluation considers a combination of spatial and spectral fidelity metrics.</p>
                    <p>Dataset access and submission details are available on the CodaBench page (link TBA)</p>
                    </p>
                </div>
            </div>
        </div>
    </div>
</div>

<div class="section" id="challenge_1">
    <div class="container">
        <div class="row">            
            <div class="panel panel-primary" id="challenge_7">
                <div class="panel-heading">
                    <h3 class="panel-title text-center">1st Unified Pedestrian Pose Estimation in Thermal Imaging Challenge (UPPET)</h3>
                </div>
                <div class="panel-body">
                    <p align="justify">In the context of human pose estimation (HPE) using thermal surveillance imagery, generalizing across different sensors is challenged by domain gaps. These gaps stem from variations in sensor specifications, environmental conditions, and calibration inconsistencies, which can affect the thermal characteristics and representations of human figures. This challenge aims to spotlight the problem of domain gaps in a real-world thermal surveillance context and highlight the challenges and limitations of existing methods to provide a direction for future research.</p>
                    <p align="justify"> In order to dynamize the fields of HPE in Thermal Imaging for realistic scenarios, we propose specially selected challenging splits for a challenge with three tracks:</p>
                    <ul>
                        <li>
                            Track 1: Generalization of HPE in Thermal Imaging using cross-validation scheme: The task is to train a pose estimator that accurately predicts persons’ keypoints under domain shifts.
                        </li>
                        <li>
                            Track 2: HPE in Thermal Imaging using specialization scheme: The task is to train a pose estimator that accurately predicts persons’ keypoints in thermal imaging.
                        </li>
                        <li>
                            Track 3: HPE on Thermal Imaging based on Synthetic Data: The task is to generate own thermal synthetic data for training. Explicitly, the participants should use generative networks (stable diffusion..etc.) to generate synthetic thermal training data. Participants must release and opensource their synthetic data. After training only on synthetic data, the pose estimator is evaluated using specialization scheme on real data.
                        </li>
                    </ul>
                    <p>Dataset access and submission details are available on the Eval.AI page (link TBA)</p>
                    </p>
                </div>
            </div>
        </div>
    </div>
</div>

<!-- <h4>Challenge Results</h4>
<p align="center">
    <img src="images/2025_Challenge_Results.png" alt="Results" style="max-width:100%; height:auto; border:1px solid #ccc; padding:5px;">
</p>
-->
</body></html>
