<!DOCTYPE html>
<html>
<head>
    <link rel="icon" href="images\logo_o_png.ico"><title>Welcome to PBVS '23</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <link href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="css/style.css" rel="stylesheet" type="text/css">
<style>
    #content li {
       margin-left:-20px;  
    }   
</style>
</head>

<body>

<div class="navbar navbar-default navbar-fixed-top" id="navi_bar">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar-ex-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="#"><span><strong>IEEE PBVS '23</strong></span></a>
        </div>
        <div class="collapse navbar-collapse" id="navbar-ex-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li ><a href="index.html"><strong>Home</strong></a></li>
                <li><a href="call_for_paper.html"><strong>Call for Paper</strong></a></li>
                <li><a href="challenge.html"><strong>Challenges</strong></a></li>
                <li><a href="submission.html"><strong>Submission</strong></a></li>
                <li ><a href="organization.html"><strong>Organization</strong></a></li>
                <li><a href="program.html"><strong>Program</strong></a></li>
                <li  class="active"><a href="keynotes.html"><strong>Keynotes</strong></a></li>
                <li ><a href="datasets.html"><strong>Datasets</strong></a></li>
                <li><a href="awards.html"><strong>Awards</strong></a></li>
                <li><a href="history.html"><strong>History</strong></a></li>
            </ul>
        </div>
    </div>
</div>

<div class="section" id="keynotes">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <div class="panel panel-primary">
                    <div class="panel-heading">
                        <h3 class="panel-title text-center">Keynotes</h3>
                    </div>
                    
                    <div class="panel-body">
                    <!--
                            <div class="row">
                            <div class="col-md-12">
                                <p align="center"><strong>Coming soon ... </strong></p>
                            </div>
                          </div> 
                    --> 
                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/jonathan_hou_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Jonathan Hou</h5> 
                                <p class="text-center">President</p>
                                <p class="text-center">Pleora Technologies</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Solving the Deployment Challenge – Moving AI from Research to the Factory Floor</h5>
                                <p align="justify"><strong>Bio:</strong>Jonathan Hou is President of Pleora Technologies, a leading supplier of AI and sensor networking solutions for the industrial automation, security & defense, and medical imaging markets. In this role, Jonathan oversees Pleora’s research & development efforts and leads the company’s long-term technology vision. Jonathan joined Pleora as Chief Technology Officer in 2018. Previously he was Director of Technology with GlobalVision, where he helped develop new automated quality inspection solutions for print inspection applications. He has held positions in software & engineering management, applications engineering, and software development in the machine vision, video, graphics and networking industries. Jonathan has a Bachelor of Applied Sciences – Computer Engineering from the University of Waterloo in Waterloo, Canada, and a Master of Engineering from McGill University in Montreal, Canada. </p>
                                <p align="justify"><strong>Abstract:</strong> Coming soon...</p>
                            </div>
                        </div>
                          
                        
                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/rikke_gade_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Rikke Gade</h5> 
                                <p class="text-center">Associate Professor</p>
                                <p class="text-center">Aalborg University, Denmark</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Thermal imaging for privacy preserving surveillance applications</h5>
                                <p align="justify"><strong>Bio:</strong>Rikke Gade is currently employed as Associate Professor at Aalborg University, Denmark, in the Visual Analysis of People Lab. She received her M.Sc. and PhD degrees from Aalborg University in 2011 and 2015, respectively. During her studies she has also visited University of Auckland, New Zealand and University of Adelaide, Australia. The PhD thesis in Computer Vision focused on analysis of activities in sports arenas; mainly occupancy analysis, activity recognition, and tracking of players. Most of her work revolves around the use of thermal video, to preserve privacy in public sports facilities. This has led to publications in top journals and international conferences within computer vision, and she has been co-organizing workshops at CVPR, ICCV, and ACCV. Her research interests include computer vision analysis of human activities, applied both for analysis of human behaviour at public spaces as well as for analysing sports activities. She also works with robot vision.</p>
                                <p align="justify"><strong>Abstract:</strong>In this talk I will dive into our story of using thermal cameras for privacy preserving computer vision algorithms at Visual Analysis and Perception lab at Aalborg University. My first encounter with thermal imaging was back in 2011 when thermal cameras were rarely seen in public computer vision research. As part of my PhD project I captured and analyzed long-term thermal datasets of a variety of human activities in both indoor and outdoor environments. The majority of our work revolves around sports applications such as occupancy analysis of sports arenas (indoor and outdoor) and analysis of sports activities where thermal cameras are used instead of regular RGB cameras to preserve privacy on public facilities. The computer vision methods applied ranges from low-level image processing and machine learning to deep learning on our more recent work. Other applications I will cover in this talk include clothing level estimation in office environments and detection of accidents in open harbour areas. During my talk I will, among other things, share insights on what to pay special attention to when using thermal cameras and demonstrate how methods designed for RGB images can be adapted to the thermal domain.</p>
                            </div>
                        </div>
                        <!--
                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/Al_Bovik_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Prof. Al Bovik</h5> 
                                <p class="text-center">Director, Laboratory for Image and Video Engineering</p>
                                <p class="text-center">University of Texas at Austin, USA</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Picture Quality Prediction Outside the Visible Spectrum </h5>
                                <p align="justify"><strong>Bio:</strong>Al Bovik is the Cockrell Family Regents Endowed Chair Professor at The University of Texas at Austin. His research interests land at the nexus of visual neuroscience and digital pictures and videos, particularly regarding how human viewers respond to visual content. An elected member of the U.S. National Academy of Engineers, his international honors include the 2022 IEEE Edison Medal, the 2019 Progress Medal of the Royal Photographic Society, the 2019 IEEE Fourier Award, the 2017 OSA Edwin H. Land Medal, a 2015 Primetime Emmy Award and a 2021 Technology and Engineering Emmy Award from the Academies of Television Arts and Sciences, and the Norbert Wiener Award of the IEEE Signal Processing Society.
                                </p>
                                <p align="justify"><strong>Abstract:</strong> In this talk I will discuss recent research on the development of no-reference or blind picture quality prediction models, for pictures captured outside of the visible spectrum. Very good algorithms already exist that accurately predict the perceptual quality of still optical pictures, most of them predicated on distortion-induced violations of natural scene statistics (NSS) models, deep learning models, or combinations of these. Here I will discuss the quality prediction of non-optical images, but in the context of task prediction on x-ray screening images. It is my hope this work will stimulate promising avenues of future research.</p>
                            </div>
                        </div>

                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/Matthew_Turek_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Dr. Matt Turek</h5> 
                                <p class="text-center">Program Manager, Information Innovation Office (I2O)</p>
                                <p class="text-center">DARPA, USA</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Media Authentication and Explainable AI: Applications Beyond the Visible Spectrum</h5>
                                <p align="justify"><strong>Bio:</strong>Dr. Matt Turek joined DARPA's Information Innovation Office (I2O) as a program manager in July 2018, and served as Acting Deputy Director of I2O from June 2021 to October 2021. Dr. Turek is the program manager for DARPA’s Explainable AI (XAI), Machine Common Sense, Semantic Forensics (SemaFor), Media Forensics (MediFor), and In the Moment (ITM) programs.
                                    Prior to his position at DARPA, Turek was at Kitware, Inc., where he led a team developing computer vision technologies. His research focused on multiple areas, including large scale behavior recognition and modeling; object detection and tracking; activity recognition; normalcy modeling and anomaly detection; and image indexing and retrieval.
                                    
                                    His research interests include computer vision, machine learning, artificial intelligence, and their application to problems with significant societal impact.
                                    
                                     
                                </p>
                                <p align="justify"><strong>Abstract:</strong>Dr. Turek’s program portfolio at DARPA includes research programs in foundational AI, such as the Machine Common Sense (MCS) and Explainable AI (XAI) programs, and programs in media authentication, such as the Media Forensics (MediFor) and Semantic Forensics (SemaFor) programs. The goal of the XAI program was to develop machine learning techniques that were more understandable and explanation interfaces that could support the explanation process with a user. MediFor and SemaFor seek to develop capabilities to detect, attribute, and characterize falsified media.

 

                                    While the goal of these programs is to build foundational capabilities, there have been applications to domains of interest to PBVS, including commercial satellite data, biological assays, and medical imaging. This talk will provide an overview of the XAI, MediFor, and SemaFor programs and highlight applications to domains of interest to PBVS.</p>
                            </div>
                        </div>

                        <!-- 
                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/Kilian_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Prof. Kilian Weinberger</h5> 
                                <p class="text-center">Associate Professor</p>
                                <p class="text-center">Cornell University, USA</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Deep Learning with Depth Perception - Representation Matters </h5>
                                <p align="justify"><strong>Bio:</strong> Kilian Weinberger is an Associate Professor in the Department of Computer Science at Cornell University. He received his Ph.D. from the University of Pennsylvania in Machine Learning and his undergraduate degree in Mathematics and Computing from the University of Oxford. During his career he has won several best paper awards, at ICML (2004), CVPR (2004, 2017), AISTATS (2005) and KDD (2014, runner-up award). In 2011 he was awarded the Outstanding AAAI Senior Program Chair Award and in 2012 he received an NSF CAREER award. He was elected co-Program Chair for ICML 2016 and for AAAI 2018. Currently he serves as an elected board member for the international machine learning society. In 2016 he was the recipient of the Daniel M Lazar '29 Excellence in Teaching Award. Kilian Weinberger's research focuses on Machine Learning and its applications, in particular on learning under resource constraints, metric learning, Gaussian Processes, computer vision and deep learning. Before joining Cornell University, he was an Associate Professor at Washington University in St. Louis and before that he worked as a research scientist at Yahoo! Research in Santa Clara. </p>
                                <p align="justify"><strong>Abstract:</strong> 3D object detection is an essential task in autonomous driving. Recent techniques excel with highly accurate detection rates, provided the 3D input data is obtained from precise but expensive LiDAR technology. Approaches based on cheaper monocular or stereo imagery data have, until recently, resulted in drastically lower accuracies - a gap that was commonly attributed to poor image-based depth estimation. In this talk we argue that it is not the precision or quality of the data, but its representation that accounts for the majority of the difference. Taking the inner workings of convolutional neural networks into consideration, we propose to convert image-based depth maps to pseudo-LiDAR representations - essentially mimicking the LiDAR signal. With this representation we can apply different existing LiDAR-based detection algorithms. Our approaches yield impressive improvements on the popular KITTI 3D object detection benchmark data set, and have already been adapted by the self-driving car community.</p>
                            </div>
                        </div>

                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/Volkan_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Prof. Volkan Cevher</h5> 
                                <p class="text-center">Associate Professor</p>
                                <p class="text-center">Swiss Federal Institute of Technology in Lausanne, Switzerland</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Adversarial machine learning: Recent developments</h5>
                                <p align="justify"><strong>Bio:</strong> Volkan Cevher received the B.Sc. (valedictorian) in electrical engineering from Bilkent University in Ankara, Turkey, in 1999 and the Ph.D. in electrical and computer engineering from the Georgia Institute of Technology in Atlanta, GA in 2005. He was a Research Scientist with the University of Maryland, College Park from 2006-2007 and also with Rice University in Houston, TX, from 2008-2009. Currently, he is an Associate Professor at the Swiss Federal Institute of Technology Lausanne and a Faculty Fellow in the Electrical and Computer Engineering Department at Rice University. His research interests include machine learning, signal processing theory, optimization, and information theory. Dr. Cevher is an ELLIS fellow and was the recipient of the Google Faculty Research Award on Machine Learning in 2018, IEEE Signal Processing Society Best Paper Award in 2016, a Best Paper Award at CAMSAP in 2015, a Best Paper Award at SPARS in 2009, and an ERC CG in 2016 as well as an ERC StG in 2011.</p>
                                <p align="justify"><strong>Abstract:</strong> Thanks to neural networks (NNs), faster computation, and massive datasets, machine learning (ML) is under increasing pressure to provide automated solutions to even harder real-world tasks beyond human performance with ever faster response times due to potentially huge technological and societal benefits. Unsurprisingly, the NN learning formulations present a fundamental challenge to the back-end learning algorithms despite their scalability, in particular due to the existence traps in the non-convex optimization landscape, such as saddle points, that can prevent algorithms to obtain “good” solutions.<br><br> In this talk, we describe our recent research that has demonstrated that the non-convex optimization dogma is false by showing that scalable stochastic optimization algorithms can avoid traps and rapidly obtain locally optimal solutions. Coupled with the progress in representation learning, such as over-parameterized neural networks, such local solutions can be globally optimal. <br><br>Unfortunately, this talk will also demonstrate that the central min-max optimization problems in ML, such as generative adversarial networks (GANs), robust reinforcement learning (RL), and distributionally robust ML, contain spurious attractors that do not include any stationary points of the original learning formulation. Indeed, we will describe how algorithms are subject to a grander challenge, including unavoidable convergence failures, which could explain the stagnation in their progress despite the impressive earlier demonstrations.</p>
                            </div>
                        </div>
                    </div>
                -->
                </div>
            </div>
        </div>
    </div>
</div>





</body></html>