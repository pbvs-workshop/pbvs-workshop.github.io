<!DOCTYPE html>
<html>
    <head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <link rel="icon" href="images/logo_o_png.ico"><title>Welcome to PBVS '25</title>
        
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet" href="./css/bootstrap.min.css">
        <script src="./css/jquery.min.js.descarga"></script>
        <script src="./css/bootstrap.min.js.descarga"></script>
        <script src="./js/scrolling-nav.js"></script>
        <link href="./css/font-awesome.min.css" rel="stylesheet" type="text/css">
        <link href="./css/style.css" rel="stylesheet" type="text/css">
        <link href="./css/scrolling-nav.css" rel="stylesheet">
    <style>
        #content li {
           margin-left:-20px;  
        }   
    </style>
    </head>

<body>

<div class="navbar navbar-default navbar-fixed-top" id="navi_bar">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar-ex-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="history.html" style="color:rgb(111,84,153);"><span><strong>21 YEARS of PBVS</strong></span></a>
        </div>
        <div class="collapse navbar-collapse" id="navbar-ex-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li ><a href="index.html"><strong>Home</strong></a></li>
                <li><a href="call_for_paper.html"><strong>Call for Paper</strong></a></li>
                <li><a href="challenge.html"><strong>Challenges</strong></a></li>
                <li><a href="submission.html"><strong>Submission</strong></a></li>
                <li ><a href="organization.html"><strong>Organization</strong></a></li>
                <li><a href="program.html"><strong>Program</strong></a></li>
                <li  class="active"><a href="keynotes.html"><strong>Keynotes</strong></a></li>
                <li ><a href="datasets.html"><strong>Datasets</strong></a></li>
                <li><a href="awards.html"><strong>Awards</strong></a></li>
                <li><a href="history.html"><strong>History</strong></a></li>
            </ul>
        </div>
    </div>
</div>

<div class="section" id="keynotes">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <div class="panel panel-primary">
                    <div class="panel-heading">
                        <h3 class="panel-title text-center">Keynotes</h3>
                    </div>
                    
                    <div class="panel-body">
                    
                        <div class="row">
                        <div class="col-md-12">
                            <!-- <p align="center"><strong>Coming soon ... </strong></p> -->
                        </div>
                        </div> 
                    </div> 

                    <div class="row">
                        <div class="col-md-4">
                            <img src="images/david_scaramuzza.jpg" class="center-block img-circle img-responsive"> 
                            <h5 class="text-center">Davide Scaramuzza</h5> 
                            <p class="text-center">Professor</p>
                            <p class="text-center">University of Zurich, Switzerland</p>
                        </div>
                        <div class="col-md-8">
                            <h5 class="text-center">Event cameras, a new way of sensing</h5>
                            <p align="justify" style="padding-right: 5%;"> <strong>Bio:</strong>Davide Scaramuzza is a Professor of Robotics and Perception at the University of Zurich. He did his Ph.D. at ETH Zurich, a postdoc at the University of Pennsylvania, and was a visiting professor at Stanford University. His research focuses on autonomous, agile microdrone navigation using standard and event-based cameras. He pioneered autonomous, vision-based navigation of drones, which inspired the navigation algorithm of the NASA Mars helicopter and many drone companies. He contributed significantly to visual-inertial state estimation, vision-based agile navigation of microdrones, and low-latency, robust perception with event cameras, which were transferred to many products, from drones to automobiles, cameras, AR/VR headsets, and mobile devices. In 2022, his team demonstrated that an AI-powered drone could outperform the world champions of drone racing, a result published in Nature and considered the first time an AI defeated a human in the physical world. He is a consultant for the United Nations on disaster response and disarmament. He has won many awards, including an IEEE Technical Field Award, the levation to IEEE Fellow, the IEEE Robotics and Automation Society Early Career Award, a European Research CouncilConsolidator Grant, a Google Research Award, two NASA TechBrief Awards, and many paper awards. In 2015, he co-founded Zurich-Eye, today Meta Zurich, which developed the world-leading virtual-reality headset Meta Quest. In 2020, he co-founded SUIND, which builds autonomous drones for precision agriculture. Many aspects of his research have been featured in the media, such as The New York Times, The Economist, and Forbes. </p>
                            <p align="justify" style="padding-right: 5%;"> <strong>Abstract:</strong>Event cameras are bio-inspired vision sensors with much lower latency, higher dynamic range, and much lower power consumption than standard cameras. This talk will present current trends and opportunities with event cameras, ranging from robotics to virtual reality and smartphones, as well as open challenges and the road ahead.</p>
                        </div>
                    </div>

                    <div class="row">
                        <div class="col-md-4">
                            <img src="images/jonathan_wu.png" class="center-block img-circle img-responsive"> 
                            <h5 class="text-center">Jonathan Wu </h5> 
                            <p class="text-center">Professor</p>
                            <p class="text-center">University of Windsor, Canada</p>
                        </div>
                        <div class="col-md-8">
                            <h5 class="text-center">Deep-Learning-Based Multisensor Data Fusion Methods and Applications</h5>
                            <p align="justify" style="padding-right: 5%;"> <strong>Bio:</strong>Dr. Jonathan Wu received a PhD in Computer Vision and Intelligent Systems from the University of Wales, UK. Dr. Wu is a Distinguished professor of electrical and computer engineering and has been a Tier 1 Canada Research Chair in Automotive Sensors and Information Systems since 2005. He is the founding director of the Computer Vision and Sensing Systems Laboratory at the University of Windsor, Canada. Prior to joining the university, Dr. Wu was a senior research officer at the National Research Council of Canada. He has published one book in the area of 3D computer vision and more than 350 peer-reviewed papers, including 200 journal articles, in the areas of computer vision, machine learning, sensor data fusion. Dr. Wu is/was an associate editor for IEEE Transactions on Cybenectics, IEEE Transactions on Circuits and Systems for Video Technology, and IEEE Transactions on Neural Networks and Learning Systems. He is an elected fellow of the Canadian Academy of Engineering. </p>
                            <p align="justify" style="padding-right: 5%;"> <strong>Abstract:</strong>Advancements in multisensor data fusion—integrating IR imaging, SAR, hyperspectral imaging, and LiDAR—have significantly enhanced object detection, classification, and scene understanding in applications such as urban monitoring, autonomous navigation, industrial diagnostics, and environmental monitoring. Deep learning-based data fusion, leveraging techniques such as convolutional neural networks (CNNs), transformers, and attention mechanisms, has demonstrated superior performance over traditional approaches in integrating heterogeneous data sources.
                                Despite these advancements, challenges such as data heterogeneity, computational complexity, and real-time processing constraints remain. Recent fusion techniques offer unique advantages in feature integration, decision-making, and robustness against sensor failures. Moreover, novel approaches such as selective sensor fusion, interleaved attention fusion, and multi-scale feature fusion have further enhanced the adaptability and accuracy of deep learning-based fusion models.
                                This keynote will explore state-of-the-art fusion methodologies and generalized inverse-based optimization, highlighting key challenges, case studies, and future research directions. The discussion will focus on scalable and efficient perception systems, addressing how deep learning and graph-based fusion strategies are transforming cybersecurity, intelligent transportation, and healthcare, among other fields. </p>
                        </div>
                    </div>
                    
<!--

                    <div class="row">
                        <div class="col-md-4">
                            <img src="images/shah_200.jpg" class="center-block img-circle img-responsive"> 
                            <h5 class="text-center">Mubarak Shah</h5> 
                            <p class="text-center">Professor</p>
                            <p class="text-center">University of Central Florida, USA</p>
                        </div>
                        <div class="col-md-8">
                            <h5 class="text-center">Beyond Visible Spectrum: Twenty Year Retrospective</h5>
                            <p align="justify"><strong>Bio:</strong>Dr. Mubarak Shah, the UCF Trustee Chair Professor, is the founding director of Center for Research in Computer Visions at University of Central Florida (UCF). Dr. Shah is a fellow of ACM, IEEE, AAAS, NAI, IAPR, AAIA and SPIE. He has published extensively on topics related to human activity and action recognition, visual tracking, geo localization, visual crowd analysis, object detection and categorization, shape from shading, etc. He has served as ACM and IEEE Distinguished Visitor Program speaker. He is a recipient of 2022 PAMI Mark Everingham Prize for pioneering human action recognition datasets; 2019 ACM SIGMM Technical Achievement award; 2020 ACM SIGMM Test of Time Honorable Mention Award for his paper “Visual attention detection in video sequences using spatiotemporal cues”; 2020 International Conference on Pattern Recognition (ICPR) Best Scientific Paper Award; an honorable mention for the ICCV 2005 Where Am I? Challenge Problem; 2013 NGA Best Research Poster Presentation; 2nd place in Grand Challenge at the ACM Multimedia 2013 conference; and runner up for the best paper award in ACM Multimedia Conference in 2005 and 2010. At UCF he has received Pegasus Professor Award; University Distinguished Research Award; Faculty Excellence in Mentoring Doctoral Students; Faculty Excellence in Mentoring Postdoctoral Scholars, Scholarship of Teaching and Learning award; Teaching Incentive Program award; and Research Incentive Award.</p>
                            <p align="justify"><strong>Abstract:</strong>I presented my talk, "Target Tracking in FLIR Imagery Using Mean-Shift and Global Motion Compensation," at the inaugural Workshop on Perception Beyond the Visible Spectrum during CVPR 2001. Since that milestone, the realm of computer vision has undergone a profound transformation, primarily fueled by the advent of deep learning. This revolution has left an indelible mark on the entire field, extending its reach beyond the boundaries of the visible spectrum. In this presentation, I aim to provide a reflective journey through the evolution of computer vision research spanning the past quarter-century. Moreover, I will delve into the transformative impact of deep learning on our understanding and exploration of perception, particularly in realms beyond what the human eye can see. Specifically, I will illuminate our endeavors in advancing perception technologies across infrared (IR), synthetic aperture radar (SAR), and light detection and ranging (LiDAR) domains. Through this retrospective lens, we will explore the fascinating interplay between technological innovation and the widening horizons of computer vision.</p>
                        </div>
                    </div>


                       <div class="row">
                            <div class="col-md-4">
                                <img src="images/ommer_bjoern_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Björn Ommer</h5> 
                                <p class="text-center">Professor</p>
                                <p class="text-center">University of Munich, Germany</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Boosting Diffusion Models for Visual Synthesis</h5>
                                <p align="justify"><strong>Bio:</strong>Björn Ommer is a full professor at Ludwig Maximilian University of Munich where he is heading the Computer Vision & Learning Group. Before, he was a full professor in the department of mathematics and computer science at Heidelberg University and a co-director of its Interdisciplinary Center for Scientific Computing. He received his diploma in computer science from University of Bonn, his PhD from ETH Zurich, and he was a postdoc at UC Berkeley. Björn serves in the Bavarian AI council and has been an associate editor for IEEE T-PAMI. His research interests include semantic scene understanding and retrieval, generative AI and visual synthesis, self-supervised metric and representation learning, and explainable AI. Moreover, he is applying this basic research in interdisciplinary projects within neuroscience and the digital humanities. His group has published a series of generative approaches, including work known as "VQGAN" and "Stable Diffusion", which are now democratizing the creation of visual content and have already opened up an abundance of new directions in research, industry, the media, and beyond.</p>
                                <p align="justify"><strong>Abstract:</strong> Recently, generative models for learning image representations have seen unprecedented progress. Approaches such as diffusion models and transformers have been widely adopted for various tasks related to visual synthesis, modification, analysis, retrieval, and beyond. Despite their enormous potential, current generative approaches have their own specific limitations. We will discuss how recently popular strategies such as flow matching can significantly enhance efficiency and democratize AI by empowering smaller models.

                                    The main part of the talk will then investigate effective ways to utilize pretrained diffusion-based image synthesis models for different tasks and modalities. Therefore, we will efficiently translate powerful generative image representations to different modalities and show evaluations on other tasks. </p>
                            </div>
                        </div>
                          
                    <!--     
                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/rikke_gade_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Rikke Gade</h5> 
                                <p class="text-center">Associate Professor</p>
                                <p class="text-center">Aalborg University, Denmark</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Beyond Visible Spectrum: Twenty Year Retrospective</h5>
                                <p align="justify"><strong>Bio:</strong>Rikke Gade is currently employed as Associate Professor at Aalborg University, Denmark, in the Visual Analysis of People Lab. She received her M.Sc. and PhD degrees from Aalborg University in 2011 and 2015, respectively. During her studies she has also visited University of Auckland, New Zealand and University of Adelaide, Australia. The PhD thesis in Computer Vision focused on analysis of activities in sports arenas; mainly occupancy analysis, activity recognition, and tracking of players. Most of her work revolves around the use of thermal video, to preserve privacy in public sports facilities. This has led to publications in top journals and international conferences within computer vision, and she has been co-organizing workshops at CVPR, ICCV, and ACCV. Her research interests include computer vision analysis of human activities, applied both for analysis of human behaviour at public spaces as well as for analysing sports activities. She also works with robot vision.</p>
                                <p align="justify"><strong>Abstract:</strong>In this talk I will dive into our story of using thermal cameras for privacy preserving computer vision algorithms at Visual Analysis and Perception lab at Aalborg University. My first encounter with thermal imaging was back in 2011 when thermal cameras were rarely seen in public computer vision research. As part of my PhD project I captured and analyzed long-term thermal datasets of a variety of human activities in both indoor and outdoor environments. The majority of our work revolves around sports applications such as occupancy analysis of sports arenas (indoor and outdoor) and analysis of sports activities where thermal cameras are used instead of regular RGB cameras to preserve privacy on public facilities. The computer vision methods applied ranges from low-level image processing and machine learning to deep learning on our more recent work. Other applications I will cover in this talk include clothing level estimation in office environments and detection of accidents in open harbour areas. During my talk I will, among other things, share insights on what to pay special attention to when using thermal cameras and demonstrate how methods designed for RGB images can be adapted to the thermal domain.</p>
                            </div>
                        </div>
                        
                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/Robert_Laganiere_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Robert Laganiere</h5> 
                                <p class="text-center">Professor</p>
                                <p class="text-center">University of Ottawa, Canada</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Perception from Radar sensors: principles and challenges</h5>
                                <p align="justify"><strong>Bio:</strong>Robert is a professor at the School of Electrical Engineering and Computer Science of the University of Ottawa and the CEO of Sensor Cortek, a startup company developing AI solutions for perception systems. Robert is the co-author of several scientific publications and patents in content-based video analysis, visual surveillance, embedded vision, driver-assistance and autonomous driving applications. Robert authored the OpenCV2 Computer Vision Application Programming Cookbook (2011) and co-authored Object Oriented Software Development (2001). He co-founded Visual Cortek in 2006, an Ottawa-based video analytics startup that was later acquired by iWatchLife in 2009. He also co-founded Tempo Analytics in 2016 a company proposing Retail Analytics solutions and founded Sensor Cortek inc in 2018.
                                </p>
                                <p align="justify"><strong>Abstract:</strong> Radar is one of the essential technologies that enables machines to perceive and interact with the world around them. By sensing the environment using radio waves, radar provides information about a scene and its objects that can be used in a wide range of applications, from autonomous driving to surveillance and security. Radar is an old technology that continues to evolve and that is expected to play an important role in the perception systems of the future. In this presentation, a survey of the radar technology and its benefit will be provided. We will explain how radar can extract range, azimuth and velocity information to detect objects and their speed. We will also explore some of the recent development that uses AI to improve radar perception and discuss the particular challenges that pose radar data in the context of deep learning.</p>
                            </div>
                        </div>
                        
                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/Zheng_Liu_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Zheng Liu</h5> 
                                <p class="text-center">Professor</p>
                                <p class="text-center">The University of British Columbia, Canada</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Towards a Comprehensive Perception: Methodologies for Thermal Imaging Data Analysis</h5>
                                <p align="justify"><strong>Bio:</strong> Zheng Liu is a professor at the Faculty of Applied Science of the University of British Columbia (UBC Okanagan). Before joining UBC, he worked for the National Research Council of Canada as a research officer and for the Toyota Technological Institute (Nagoya) as a professor. His research interests include digital twin, data/information fusion, computer/machine vision, machine learning, smart sensor and industrial IoT, and non-destructive inspection and evaluation. Dr. Liu is a fellow of SPIE and a senior member of IEEE and holds Professional Engineer licenses in both British Columbia and Ontario. In addition, Dr. Liu serves on the editorial boards for journals including Information Fusion (Elsevier), Machine Vision and Applications (Springer), IEEE Transactions on Instrumentation and Measurement, IEEE Transactions on AgriFood Electronics, and IEEE Journal of RFID, and CAAI Transactions on intelligence technology.</p>
                                <p align="justify"><strong>Abstract:</strong> Many industrial sectors and applications have benefited from thermal imaging technology for its perception capability enabled by its spectrum and advances in computational methodologies in varied situations. A thermal imaging system can be configured or operated in different modes, e.g., unimodal and multimodal, with or without explicit fusion operations. However, what thermal imaging contributes to human perception depends on the forms of the derived information, i.e., how the thermal imaging data are processed with the auxiliary data and information. This talk will overview the methodologies for processing and analyzing thermal imaging data in the context of application needs. The research opportunities and challenges will be highlighted in the presentation.</p>
                                </div>
                        </div>

                
                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/Kilian_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Prof. Kilian Weinberger</h5> 
                                <p class="text-center">Associate Professor</p>
                                <p class="text-center">Cornell University, USA</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Deep Learning with Depth Perception - Representation Matters </h5>
                                <p align="justify"><strong>Bio:</strong> Kilian Weinberger is an Associate Professor in the Department of Computer Science at Cornell University. He received his Ph.D. from the University of Pennsylvania in Machine Learning and his undergraduate degree in Mathematics and Computing from the University of Oxford. During his career he has won several best paper awards, at ICML (2004), CVPR (2004, 2017), AISTATS (2005) and KDD (2014, runner-up award). In 2011 he was awarded the Outstanding AAAI Senior Program Chair Award and in 2012 he received an NSF CAREER award. He was elected co-Program Chair for ICML 2016 and for AAAI 2018. Currently he serves as an elected board member for the international machine learning society. In 2016 he was the recipient of the Daniel M Lazar '29 Excellence in Teaching Award. Kilian Weinberger's research focuses on Machine Learning and its applications, in particular on learning under resource constraints, metric learning, Gaussian Processes, computer vision and deep learning. Before joining Cornell University, he was an Associate Professor at Washington University in St. Louis and before that he worked as a research scientist at Yahoo! Research in Santa Clara. </p>
                                <p align="justify"><strong>Abstract:</strong> 3D object detection is an essential task in autonomous driving. Recent techniques excel with highly accurate detection rates, provided the 3D input data is obtained from precise but expensive LiDAR technology. Approaches based on cheaper monocular or stereo imagery data have, until recently, resulted in drastically lower accuracies - a gap that was commonly attributed to poor image-based depth estimation. In this talk we argue that it is not the precision or quality of the data, but its representation that accounts for the majority of the difference. Taking the inner workings of convolutional neural networks into consideration, we propose to convert image-based depth maps to pseudo-LiDAR representations - essentially mimicking the LiDAR signal. With this representation we can apply different existing LiDAR-based detection algorithms. Our approaches yield impressive improvements on the popular KITTI 3D object detection benchmark data set, and have already been adapted by the self-driving car community.</p>
                            </div>
                        </div>

                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/Volkan_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Prof. Volkan Cevher</h5> 
                                <p class="text-center">Associate Professor</p>
                                <p class="text-center">Swiss Federal Institute of Technology in Lausanne, Switzerland</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Adversarial machine learning: Recent developments</h5>
                                <p align="justify"><strong>Bio:</strong> Volkan Cevher received the B.Sc. (valedictorian) in electrical engineering from Bilkent University in Ankara, Turkey, in 1999 and the Ph.D. in electrical and computer engineering from the Georgia Institute of Technology in Atlanta, GA in 2005. He was a Research Scientist with the University of Maryland, College Park from 2006-2007 and also with Rice University in Houston, TX, from 2008-2009. Currently, he is an Associate Professor at the Swiss Federal Institute of Technology Lausanne and a Faculty Fellow in the Electrical and Computer Engineering Department at Rice University. His research interests include machine learning, signal processing theory, optimization, and information theory. Dr. Cevher is an ELLIS fellow and was the recipient of the Google Faculty Research Award on Machine Learning in 2018, IEEE Signal Processing Society Best Paper Award in 2016, a Best Paper Award at CAMSAP in 2015, a Best Paper Award at SPARS in 2009, and an ERC CG in 2016 as well as an ERC StG in 2011.</p>
                                <p align="justify"><strong>Abstract:</strong> Thanks to neural networks (NNs), faster computation, and massive datasets, machine learning (ML) is under increasing pressure to provide automated solutions to even harder real-world tasks beyond human performance with ever faster response times due to potentially huge technological and societal benefits. Unsurprisingly, the NN learning formulations present a fundamental challenge to the back-end learning algorithms despite their scalability, in particular due to the existence traps in the non-convex optimization landscape, such as saddle points, that can prevent algorithms to obtain “good” solutions.<br><br> In this talk, we describe our recent research that has demonstrated that the non-convex optimization dogma is false by showing that scalable stochastic optimization algorithms can avoid traps and rapidly obtain locally optimal solutions. Coupled with the progress in representation learning, such as over-parameterized neural networks, such local solutions can be globally optimal. <br><br>Unfortunately, this talk will also demonstrate that the central min-max optimization problems in ML, such as generative adversarial networks (GANs), robust reinforcement learning (RL), and distributionally robust ML, contain spurious attractors that do not include any stationary points of the original learning formulation. Indeed, we will describe how algorithms are subject to a grander challenge, including unavoidable convergence failures, which could explain the stagnation in their progress despite the impressive earlier demonstrations.</p>
                            </div>
                        </div>
                    -->
                </div>
                
                </div>
            </div>
        </div>
    </div>
</div>





</body></html>
