<!DOCTYPE html>
<html>
<head>
    <link rel="icon" href="images\logo_o_png.ico"><title>Welcome to PBVS '20</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <link href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="css/style.css" rel="stylesheet" type="text/css">
<style>
    #content li {
       margin-left:-20px;  
    }   
</style>
</head>

<body>

<div class="navbar navbar-default navbar-fixed-top" id="navi_bar">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar-ex-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="#"><span><strong>IEEE PBVS '20</strong></span></a>
        </div>
        <div class="collapse navbar-collapse" id="navbar-ex-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li ><a href="index.html"><strong>Home</strong></a></li>
                <li><a href="call_for_paper.html"><strong>Call for Paper</strong></a></li>
                <li><a href="challenge.html"><strong>Challenge</strong></a></li>
                <li><a href="submission.html"><strong>Submission</strong></a></li>
                <!--
                <li class="dropdown active">
                    <a class="dropdown-toggle" data-toggle="dropdown" href="#"><strong>Authors</strong><span class="caret"></span></a>
                    <ul class="dropdown-menu">
                      <li class="active"><a href="call_for_paper.html"><strong>Call for Paper</strong></a></li>
                      <li><a href="submission.html"><strong>Submission</strong></a></li>
                    </ul>
                  </li>
                -->
                <li ><a href="organization.html"><strong>Organization</strong></a></li>
                <li><a href="program.html"><strong>Program</strong></a></li>
                <li  class="active"><a href="keynotes.html"><strong>Keynotes</strong></a></li>
                <li ><a href="datasets.html"><strong>Datasets</strong></a></li>
                <li><a href="awards.html"><strong>Awards</strong></a></li>
                <li><a href="history.html"><strong>History</strong></a></li>
            </ul>
        </div>
    </div>
</div>

<div class="section" id="keynotes">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <div class="panel panel-primary">
                    <div class="panel-heading">
                        <h3 class="panel-title text-center">Keynotes</h3>
                    </div>
                    
                    <div class="panel-body">
                       <!--
                       <div class="row">
                            <div class="col-md-12">
                                <p align="center"><strong>Coming soon</strong></p>
                            </div>
                       </div>
                        -->
                         
                       <div class="row">
                            <div class="col-md-4">
                                <img src="images/Michael_Felsberg_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Michael Felsberg</h5> 
                                <p class="text-center">Professor</p>
                                <p class="text-center">Linköping University, Sweden</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Learning to Analyze what is Beyond the Visible Spectrum</h5>
                                <p align="justify"><strong>Bio:</strong> Michael Felsberg (MSc 1998, PhD 2002) is Full Professor and the Head of the Computer Vision Laboratory, Linköping University. His research interests include learning and modeling of machine perception. He has published more than 200 reviewed conference papers, journal articles, and book contributions, with in total more than 11,000 citations. He has received several awards, among others from the German Pattern Recognition Society in 2000, 2004, and 2005, from the Swedish Society for Automated Image Analysis in 2007 and 2010, from the CVPR Workshop on Mobile Vision 2014, and from ICPR 2016 (best paper in computer vision). He has achieved top ranks on various challenges (VOT: 3rd 2013, 1st 2014, 2nd 2015; VOT-TIR: 1st 2015; OpenCV Tracking: 1st 2015; KITTI Stereo Odometry: 1st 2015, March). He is regularly Associate Editor for major journals in the field (e.g. JMIV, IMAVIS) and Area Chair for top-tier conferences (e.g. ECCV, BMVC, CVPR). He was Track Chair of the International Conference on Pattern Recognition 2016, General Co-Chair of the DAGM symposium in 2011, General Chair of CAIP 2017, and Program Chair of SCIA 2019.</p>
                                <p align="justify"><strong>Abstract:</strong> Coming soon...</p>
                                
                            </div>
                        </div>
                        
                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/Bamler_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Richard Bamler</h5> 
                                <p class="text-center">Director IMF</p>
                                <p class="text-center">German Aerospace Center (DLR)</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Synthetic Aperture Radar is More Than Imaging <font color="red">(Cancelled)</font></h5>
                                <p align="justify"><strong>Bio:</strong>Richard Bamler studied Electrical Engineering and received his PhD and his ‘Habilitation’ in 1986 and 1988, respectively, from the Technical University of Munich (TUM) where he worked on optical signal processing, holography and wave propagation. He joined the German Aerospace Center (DLR) in 1989, where he is currently the Director of the Remote Sensing Technology Institute. In 1994 and 1996, respectively, Richard Bamler was a visiting scientist at NASA-JPL and a guest professor at the University of Innsbruck. Since 2003 he has been full professor at TUM as a double appointment with his DLR position. His research interests are in algorithms for optimum information extraction from remote sensing data. This involves new estimation algorithms, like sparse reconstruction and compressive sensing for SAR, multi-/hyperspectral imaging, data fusion and machine learning. Richard Bamler, his team, and his institute have been developing algorithms and operational processing systems for almost all German and many European Earth observation satellite missions, covering a wide range of remote sensing technologies from radar and optical imaging to atmospheric sounding.</p>
                                <p align="justify"><strong>Abstract:</strong> Coming soon...</p>
                            </div>
                        </div>
                        
                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/Kilian_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Kilian Weinberger</h5> 
                                <p class="text-center">Associate Professor</p>
                                <p class="text-center">Cornell University, USA</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Deep Learning with Depth Perception - Representation Matters <font color="red">(Cancelled)</font></h5>
                                <p align="justify"><strong>Bio:</strong> Kilian Weinberger is an Associate Professor in the Department of Computer Science at Cornell University. He received his Ph.D. from the University of Pennsylvania in Machine Learning and his undergraduate degree in Mathematics and Computing from the University of Oxford. During his career he has won several best paper awards, at ICML (2004), CVPR (2004, 2017), AISTATS (2005) and KDD (2014, runner-up award). In 2011 he was awarded the Outstanding AAAI Senior Program Chair Award and in 2012 he received an NSF CAREER award. He was elected co-Program Chair for ICML 2016 and for AAAI 2018. Currently he serves as an elected board member for the international machine learning society. In 2016 he was the recipient of the Daniel M Lazar '29 Excellence in Teaching Award. Kilian Weinberger's research focuses on Machine Learning and its applications, in particular on learning under resource constraints, metric learning, Gaussian Processes, computer vision and deep learning. Before joining Cornell University, he was an Associate Professor at Washington University in St. Louis and before that he worked as a research scientist at Yahoo! Research in Santa Clara. </p>
                                <p align="justify"><strong>Abstract:</strong> Coming soon...</p>
                            </div>
                        </div>
                        
                        <!-- 
                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/Sabine_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Prof. Sabine Süsstrunk</h5> 
                                <p class="text-center">Professor</p>
                                <p class="text-center">EPFL, Switzerland</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">RGB+: Using Near-Infrared (NIR) to improve Computational Photography Applications</h5>
                                <p align="justify"><strong>Bio:</strong> <u><a class="content" target="_blank" href="https://ivrl.epfl.ch/people/susstrunk">Sabine Süsstrunk</a></u> is full professor in the School of Information and Communication Sciences (IC) at the Ecole Polytechnique Fédérale de Lausanne (EPFL), Switzerland, where she leads the Images and Visual Representation Lab since 1999. Her research areas are in computational photography, color computer vision and color image processing, image quality, and computational aesthetics. She has published over 150 scientific papers, of which 7 have received best paper/demos awards (ACM Multimedia 2010, IS&T CIC 2012, IEEE ICIP 2013, etc.), and holds 10 patents. In 2013, she received the IS&T/SPIE Electronic Imaging Scientist of the Year Award. She is a Fellow of IEEE and IS&T. </p>
                                <p align="justify"><strong>Abstract:</strong> Conventional digital cameras exhibit a number of limitations that computational photography systems try to overcome. For example, the disambiguation of how much the illuminant(s) and the object reflectance contribute to a pixel value is mathematically ill-posed. Given how most modern cameras capture images, blur and limited depth-of-field may also introduce noise and unwanted artifacts. To solve this problem, experts have proposed modified hardware, smart algorithms using priors, and (deep) machine learning approaches. In our research, we use "extra information" in the form of near-infrared (NIR), the wavelength range adjacent to the visible spectrum and easily captured by conventional silicon sensors. Capturing NIR can improve computational photography tasks such as dehazing, white-balancing, shadow detection, deblurring, and depth-of-field extension, as well as computer vision applications such as detection and classification.</p>
                            </div>
                        </div>
                        -->
                        <!-- 
                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/Priddy_Kevin.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Dr. Kevin Priddy</h5> 
                                <p class="text-center">Chief Systems Engineer</p>
                                <p class="text-center">Air Force DCGS, USA</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">The Hype, Hope and Promise of Learning Machines</h5>
                                
                                <p align="justify"><strong>Bio:</strong> Kevin Priddy is the Chief Systems Engineer for the Air Force Distributed Common Ground Station (AF DCGS). The AF DCGS, also referred to as the AN/GSQ-272 SENTINEL weapon system, is the Air Force’s primary intelligence, surveillance and reconnaissance (ISR) collection, processing, exploitation, analysis and dissemination (CPAD) system.
                                </p>
                                <p align="justify">He received the Electrical Engineering degree from Brigham Young University in 1982 and entered the USAF as a an engineering officer.  In 1985 he received his MS in electrical engineering from the Air Force Institute of Technology focusing on electro-optics and semiconductor theory.  He entered the Air Force Institute of Technology in 1989 to pursue his PhD in 1989 and graduated with his PhD degree in Electrical Engineering focused on machine learning and pattern recognition n 1992.  Shortly after receiving his PhD, he left the Air Force and joined Accurate Automation Corporation which was focused on utilizing artificial neural networks on a variety of problems of interest for industry and the DoD.  While at AAC he was the co-inventor of a digital neural network processor which in its day outperformed all of the other neural network processors on the market.  1997 he joined the Cognitive Systems Initiative at Battelle Memorial Institute where he became its director in 1999  until 2002.  In 2002 he joined Jacobs Sverdrup in Dayton Ohio to assess how well automated target recognition systems were performing that were developed with the DARPA MSTAR project.  In 2005 he rejoined the USAF as a civilian in the Air Force Research Laboratory (AFRL) Sensors Directorate and began working with the development of wide area motion imagery (WAMI) systems.</p>
                                <p align="justify">He is a nationally recognized expert in the conceptual formulation, development, and transition of airborne WAMI  systems and their associated layered sensing environments for processing exploitation and dissemination of WAMI data. He was the chief engineer for the AngelFire project which fielded in Iraq and for its follow-on Blue Devil, which fielded in Afghanistan,  as well as the driving force behind the Pursuer three-dimensional multi-int layered sensing viewer used to exploit ISR data products. His national leadership in these areas has resulted in significant investments from around the DoD to improve the nation’s capability to perform ISR missions in support of counterinsurgency (COIN) operations in Iraq and Afghanistan and laid the foundation for future advancements in ISR capabilities. He has co-authored one book on neural networks, over 50 technical papers and is a co-inventor of three patents.  He is a fellow of SPIE and AFRL for his contributions in machine learning and real-time wide area motion imagery.</p>
                                
                                <p align="justify"><strong>Abstract:</strong>  Machine learning and deep learning in particular have been front and center in a resurgence of data analytics and pattern recognition tools in the past few years.  Tremendous advances have been made in one shot learning, face recognition, pattern recognition and even scene understanding.   This success has created a massive resurgence in researchers promising the solution to a myriad of new problems.</p>
                                <p align="justify">This presentation will focus on the hype, hope and promise of deep learning and compare where deep learning is today to previous renaissance periods in machine learning.  There are a number of lessons that have been learned over the years which many who have not studied the past may repeat. </p>
                                <p align="justify">Two years ago Yan Le Cun pointed out a number of weaknesses in deep learning when applied to real world problems.  This presentation will present a practitioner’s point of view of working with learning systems to solve problems of interest to the DoD.  Over the years there have been many proclaiming that a new algorithm or learning system was going to solve significant problems of interest in one area only to fail miserably when applied to a different problem in the same domain or in a different domain altogether.</p>
                                <p align="justify">This presentation will highlight lessons learned over a long period of working with learning systems and will hopefully show the audience pitfalls to avoid as they move the state-of-the art forward.  Deep learning is making significant strides but we must temper our enthusiasm with reality and address the shortcomings to create really useful learning systems.</p>
                                
                            </div>
                        </div>
                        
                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/jingyi_yu.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Prof. Jingyi Yu</h5> 
                                <p class="text-center">Professor</p>
                                <p class="text-center">University of Delaware</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center"> Towards Ultimate Plenoptic Imaging</h5>
                                <p align="justify"><strong>Bio:</strong>Jingyi Yu is a Professor at the School of Information Science and Technology at the ShanghaiTech University and a Professor in the Department of Computer and Information Sciences at the University of Delaware. He is also the founding director of the Virtual Reality and Visual Computing Center at ShanghaiTech. Yu received B.S. from Caltech in 2000 and Ph.D. from MIT in 2005. His research interests span a range of areas in computer vision, computational photography and computer graphics and he is a recipient of the NSF CAREER Award and the AFOSR YIP Award. He was a Program Chair of ICCP 2016 and an Area Chair of ICCV '11,'15, '17, CVPR '17, and NIPS '15, '17. He also serves as an Associate Editor of IEEE TPAMI, Elsevier CVIU, Springer TVCJ and Springer MVA. In 2015, he founded Plex VR, a startup company that focuses on light field virtual reality and augmented reality.​<p>
                                <p align="justify"><strong>Abstract:</strong> The complete plenoptic function records radiance of rays from every location, at every angle, for every wavelength and at every time. Existing techniques have been focused on reconstructing a subset of different dimensions: a static camera for spatial, a video camera for spatial and temporal, a light field camera for spatial and angular, and imaging spectrometer for spatial and spectral. Simultaneously capturing multiple dimensions of the plenoptic function is challenging and requires special imaging systems and post-processing algorithms. In this talk, I present several latest solutions towards ultimate plenoptic imaging. I first demonstrate a hyperspectral light field (H-LF) camera array, with each camera equipped with a narrow bandpass filter centered at a specific wavelength. To fuse H-LF data, I present a new spectral-invariant feature descriptor and its companion matching metric to maintain robustness and accuracy. Based on the new metric, we further tailor a H-LF stereo matching scheme that employs a spectral-dependent data cost and a spectral-aware defocus cost for high fidelity depth estimation. The camera array system is effective but bulky. Therefore, I further present a single camera H-LF solution called Snapshot Plenoptic Imager (SPI) that uses spectral coded catadioptric mirror arrays for simultaneously acquiring the spatial, angular and spectral dimensions. We show that the spectral signal exhibits the sparsity property and we apply a learning-based approach to improve the spectral resolution from limited measurements. Finally, I demonstrate using the recovered plenoptic function for a variety of applications, ranging from video surveillance to color image synthesis and hyperspectral refocusing.</p>
                                
                            </div>
                        </div>
                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/Leo_headshot.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Leo Tam</h5> 
                                <p class="text-center">Senior Solutions Architect</p>
                                <p class="text-center">NVIDIA, USA</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Practical Deep Learning: Studies from Medical Imaging</h5>
                                <p align="justify"><strong>Bio:</strong> Leo Tam serves as the Deep Learning (DL) Senior Solutions Architect, part of the NALA-based, World Wide Field Organization (WWFO). Leo Tam brings to NVIDIA 10 years of R&D experience, most recently as a Postdoctoral Research Scientist at Stanford’s School of Medicine, where he applied deep learning technologies for automatic diabetic retinopathy diagnosis using high resolution images of the human retina. The resulting algorithm placed in the top 3% of a world-wide competition sponsored by the California Healthcare Foundation. Leo holds multiple patents for MRI encoding methods while serving as a Research Scientist at Resonance Research, Inc. and the Yale University School of Medicine. While at Yale, Leo researched, developed, and published a statistical machine learning technique to detect sparse features from MRI images. Leo earned his BSc in Mathematics-Physics from Brown University and PhD from Yale University.</p>
                                <p align="justify"><strong>Abstract:</strong> Medical imaging is a rich field from which to draw nonvisible spectrum imaging data. A data scientist working on such datasets may seek open source tools and quickly face a myriad of options. In the talk, I present three recent projects and extract learnings for hardware and software tools to build and deploy models. The three projects include volumetric cardiac segmentation for ejection fraction prediction, fully unsupervised clustering for high-content microscopy images, and retinal imaging for retinopathy screening. Respectively, they investigate novel deep learning approaches for semantic segmentation, unsupervised learning, and application specific network architectures. The segmentation application shows the importance of labeled data and extracting performance from data sources. The clustering analysis reveals how deep learning can bring end-to-end learning to traditional machine learning domains. Finally, patch-based architectures are used to increase accuracy for image classification. Software tools surveyed and used in our projects include NVIDIA DIGITS, BVLC Caffe framework, NYU’s Torch7 framework, and Google’s Tensorflow implementation. The lessons gleaned from these case studies help scope the dataset requirements, engineering economy, and application feasibility. Emerging from the case studies are a series for recommendations that may constitute best practices for academic and industrial deep learning applied to disparate imaging modalities and applications.</p>
                             
                                
                            </div>
                        </div>
                    
                    </div>
                    -->
                </div>
            </div>
        </div>
    </div>
</div>





</body></html>