<!DOCTYPE html>
<html>
    <head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <link rel="icon" href="images/logo_o_png.ico"><title>Welcome to PBVS '26</title>
        
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet" href="./css/bootstrap.min.css">
        <script src="./css/jquery.min.js.descarga"></script>
        <script src="./css/bootstrap.min.js.descarga"></script>
        <script src="./js/scrolling-nav.js"></script>
        <link href="./css/font-awesome.min.css" rel="stylesheet" type="text/css">
        <link href="./css/style.css" rel="stylesheet" type="text/css">
        <link href="./css/scrolling-nav.css" rel="stylesheet">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
        <link href="css/modern.css" rel="stylesheet" type="text/css">
    <style>
        #content li {
           margin-left:-20px;
        }
    </style>
    </head>

<body>

<div class="navbar navbar-default navbar-fixed-top" id="navi_bar">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar-ex-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="history.html" style="color:rgb(111,84,153);"><span><strong>22 YEARS of PBVS</strong></span></a>
        </div>
        <div class="collapse navbar-collapse" id="navbar-ex-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li ><a href="index.html"><strong>Home</strong></a></li>
                <li><a href="call_for_paper.html"><strong>Call for Paper</strong></a></li>
                <li><a href="challenge.html"><strong>Challenges</strong></a></li>
                <li><a href="submission.html"><strong>Submission</strong></a></li>
                <li ><a href="organization.html"><strong>Organization</strong></a></li>
                <li><a href="program.html"><strong>Program</strong></a></li>
                <li  class="active"><a href="keynotes.html"><strong>Keynotes</strong></a></li>
                <li ><a href="datasets.html"><strong>Datasets</strong></a></li>
                <li><a href="awards.html"><strong>Awards</strong></a></li>
                <li><a href="history.html"><strong>History</strong></a></li>
            </ul>
        </div>
    </div>
</div>

<div class="section" id="keynotes">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <div class="panel panel-primary">
                    <div class="panel-heading">
                        <h3 class="panel-title text-center">Keynotes</h3>
                    </div>
                    
                    <div class="panel-body">
                    
                        <div class="row">
                        <div class="col-md-12">
                        </div>
                        </div> 
                    </div> 

                    <div class="row">
                        <div class="col-md-4">
                            <img src="images/brian_sheil.jpg" class="center-block img-circle img-responsive"> 
                            <h5 class="text-center">Brian Sheil</h5> 
                            <p class="text-center">Professor</p>
                            <p class="text-center">University of Cambridge, UK</p>
                        </div>
                        <div class="col-md-8">
                            <h5 class="text-center">The Seen and the Unseen: Multi-Modal Sensing for Understanding Surface and Subsurface Infrastructure</h5>
                            <p align="justify" style="padding-right: 5%;"> <strong>Bio:</strong>Brian Sheil is the Director of the Centre for Smart Infrastructure and Construction at University of Cambridge. He previously held academic positions at the University of Oxford and was awarded a Royal Academy of Engineering Research Fellowship before moving to Cambridge in 2022 to take up the Laing O’Rourke Associate Professorship in Construction Engineering. In 2024, he was awarded an EPSRC Open Fellowship for his work on Digital Underground Construction. He is a co-founder and Chief Scientist of the startup InfraMind, which develops AI-based “digital inspectors” for infrastructure, currently being trialled by organisations including National Highways, Network Rail, and Transport for London. He serves on the editorial boards of several leading journals and has received multiple awards for research excellence and commercialisation. His research puts physics first in developing trustworthy AI for infrastructure, combining computer vision, multimodal sensing, physics-informed learning, and multi-scale simulation to enable digital twins, inspection, early warning, and lifecycle management of underground and civil infrastructure. </p>
                            <!--<p align="justify" style="padding-right: 5%;"> <strong>Abstract:</strong>Event cameras are bio-inspired vision sensors with much lower latency, higher dynamic range, and much lower power consumption than standard cameras. This talk will present current trends and opportunities with event cameras, ranging from robotics to virtual reality and smartphones, as well as open challenges and the road ahead.</p> -->
                            <p align="justify" style="padding-right: 5%;"> <strong>Abstract:</strong>TBA</p>
                        </div>
                    </div>
                    
                    <div class="row">
                        <div class="col-md-4">
                            <img src="images/sarah_parisot.jpg" class="center-block img-circle img-responsive"> 
                            <h5 class="text-center">Sarah Parisot </h5> 
                            <p class="text-center">Principal Research Manager</p>
                            <p class="text-center">Microsoft, USA</p>
                        </div>
                        <div class="col-md-8">
                            <h5 class="text-center">Building World Models for Creative Use</h5>
                            <p align="justify" style="padding-right: 5%;"> <strong>Bio:</strong>Dr. Sarah Parisot is a Principal Research Manager in the People-Centric AI group at Microsoft Research Cambridge. Her research interests include data efficient learning, large vision-language models, and controllability of visual generative models, including interactive world models and their application to support creative ideation. Prior to joining Microsoft, she led the London AI Theory team at Huawei’s Noah’s Ark Lab. Earlier in her career, she was a Research Associate at Imperial College London, where she developed graph neural network methodologies for medical imaging. She earned her PhD in Applied Mathematics from INRIA and École Centrale Paris in 2013.</p>
                            <p align="justify" style="padding-right: 5%;"> <strong>Abstract:</strong>World models offer a path toward interactive, co‑creative systems that support iteration, exploration, and sustained creative control. To be useful to creators, such models must balance expressiveness with practical constraints such as data efficiency, responsiveness, and inference cost. This talk explores the interplay between model design and creative intent, including how representation choices, efficiency techniques, and data strategy can shape creative use. </p>
                        </div>
                    </div>

                    <!--
                    <div class="row">
                        <div class="col-md-4">
                            <img src="images/WilliamFreeman_332x332.jpg" class="center-block img-circle img-responsive"> 
                            <h5 class="text-center">William Freeman </h5> 
                            <p class="text-center">Professor</p>
                            <p class="text-center">Massachusetts Institute of Technology</p>
                        </div>
                        <div class="col-md-8">
                            <h5 class="text-center">Exploiting the assumption of source and sensor independence</h5>
                            <p align="justify" style="padding-right: 5%;"> <strong>Bio:</strong>William T. Freeman is the Thomas and Gerd Perkins Professor of Electrical Engineering and Computer Science (EECS) at MIT, and a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL) there.  He was the Associate Department Head of EECS from 2011 – 2014.  Since 2015, he has also been a research manager in Google Research in Cambridge, MA. His current research interests include mid-level vision and computational photography.  Previous research topics include steerable filters and pyramids, orientation histograms, the generic viewpoint assumption, color constancy, computer vision for computer games, motion magnification, and belief propagation in networks with loops.  He received outstanding paper awards at computer vision or machine learning conferences in 1997, 2006, 2009,  2012 and 2019, and test-of-time awards for papers from 1990, 1995, 2002, 2005, and 2012.    He shared the 2020 Breakthrough Prize in Physics for a consulting role with the Event Horizon Telescope collaboration, which reconstructed the first image of a black hole.  He is a member of the National Academy of Engineering, and a Fellow of the IEEE, ACM, and AAAI.   In 2019, he received the PAMI Distinguished Researcher Award, the highest award in computer vision. </p>
                            <p align="justify" style="padding-right: 5%;"> <strong>Abstract:</strong>  This talk addresses the general problem of source estimation from underconstrained linear inverse problems--estimating x from the observations y = A x, where the measurement matrix A has more columns than rows. Very often, x and A are independent of each other and that fact adds important information that may allow us to lessen our reliance on prior assumptions about x. Exploiting that assumption may be useful in applications in science and medicine.</p>
                        </div>
                    </div>
                    -->
                </div>
                
                </div>
            </div>
        </div>
    </div>
</div>


</body></html>
