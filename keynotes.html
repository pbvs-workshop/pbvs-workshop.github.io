<!DOCTYPE html>
<html>
    <head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <link rel="icon" href="images/logo_o_png.ico"><title>Welcome to PBVS '25</title>
        
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet" href="./css/bootstrap.min.css">
        <script src="./css/jquery.min.js.descarga"></script>
        <script src="./css/bootstrap.min.js.descarga"></script>
        <script src="./js/scrolling-nav.js"></script>
        <link href="./css/font-awesome.min.css" rel="stylesheet" type="text/css">
        <link href="./css/style.css" rel="stylesheet" type="text/css">
        <link href="./css/scrolling-nav.css" rel="stylesheet">
    <style>
        #content li {
           margin-left:-20px;  
        }   
    </style>
    </head>

<body>

<div class="navbar navbar-default navbar-fixed-top" id="navi_bar">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar-ex-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="history.html" style="color:rgb(111,84,153);"><span><strong>21 YEARS of PBVS</strong></span></a>
        </div>
        <div class="collapse navbar-collapse" id="navbar-ex-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li ><a href="index.html"><strong>Home</strong></a></li>
                <li><a href="call_for_paper.html"><strong>Call for Paper</strong></a></li>
                <li><a href="challenge.html"><strong>Challenges</strong></a></li>
                <li><a href="submission.html"><strong>Submission</strong></a></li>
                <li ><a href="organization.html"><strong>Organization</strong></a></li>
                <li><a href="program.html"><strong>Program</strong></a></li>
                <li  class="active"><a href="keynotes.html"><strong>Keynotes</strong></a></li>
                <li ><a href="datasets.html"><strong>Datasets</strong></a></li>
                <li><a href="awards.html"><strong>Awards</strong></a></li>
                <li><a href="history.html"><strong>History</strong></a></li>
            </ul>
        </div>
    </div>
</div>


<h2>Coming soon</h2>
<!--

<div class="section" id="keynotes">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <div class="panel panel-primary">
                    <div class="panel-heading">
                        <h3 class="panel-title text-center">Keynotes</h3>
                    </div>
                    
                    <div class="panel-body">
                    
                        <div class="row">
                        <div class="col-md-12">
                        </div>
                        </div> 
                    </div> 

                    <div class="row">
                        <div class="col-md-4">
                            <img src="images/david_scaramuzza.jpg" class="center-block img-circle img-responsive"> 
                            <h5 class="text-center">Davide Scaramuzza</h5> 
                            <p class="text-center">Professor</p>
                            <p class="text-center">University of Zurich, Switzerland</p>
                        </div>
                        <div class="col-md-8">
                            <h5 class="text-center">Event cameras, a new way of sensing</h5>
                            <p align="justify" style="padding-right: 5%;"> <strong>Bio:</strong>Davide Scaramuzza is a Professor of Robotics and Perception at the University of Zurich. He did his Ph.D. at ETH Zurich, a postdoc at the University of Pennsylvania, and was a visiting professor at Stanford University. His research focuses on autonomous, agile microdrone navigation using standard and event-based cameras. He pioneered autonomous, vision-based navigation of drones, which inspired the navigation algorithm of the NASA Mars helicopter and many drone companies. He contributed significantly to visual-inertial state estimation, vision-based agile navigation of microdrones, and low-latency, robust perception with event cameras, which were transferred to many products, from drones to automobiles, cameras, AR/VR headsets, and mobile devices. In 2022, his team demonstrated that an AI-powered drone could outperform the world champions of drone racing, a result published in Nature and considered the first time an AI defeated a human in the physical world. He is a consultant for the United Nations on disaster response and disarmament. He has won many awards, including an IEEE Technical Field Award, the levation to IEEE Fellow, the IEEE Robotics and Automation Society Early Career Award, a European Research CouncilConsolidator Grant, a Google Research Award, two NASA TechBrief Awards, and many paper awards. In 2015, he co-founded Zurich-Eye, today Meta Zurich, which developed the world-leading virtual-reality headset Meta Quest. In 2020, he co-founded SUIND, which builds autonomous drones for precision agriculture. Many aspects of his research have been featured in the media, such as The New York Times, The Economist, and Forbes. </p>
                            <p align="justify" style="padding-right: 5%;"> <strong>Abstract:</strong>Event cameras are bio-inspired vision sensors with much lower latency, higher dynamic range, and much lower power consumption than standard cameras. This talk will present current trends and opportunities with event cameras, ranging from robotics to virtual reality and smartphones, as well as open challenges and the road ahead.</p>
                        </div>
                    </div>

                    <div class="row">
                        <div class="col-md-4">
                            <img src="images/jonathan_wu.png" class="center-block img-circle img-responsive"> 
                            <h5 class="text-center">Jonathan Wu </h5> 
                            <p class="text-center">Professor</p>
                            <p class="text-center">University of Windsor, Canada</p>
                        </div>
                        <div class="col-md-8">
                            <h5 class="text-center">Deep-Learning-Based Multisensor Data Fusion Methods and Applications</h5>
                            <p align="justify" style="padding-right: 5%;"> <strong>Bio:</strong>Dr. Jonathan Wu received a PhD in Computer Vision and Intelligent Systems from the University of Wales, UK. Dr. Wu is a Distinguished professor of electrical and computer engineering and has been a Tier 1 Canada Research Chair in Automotive Sensors and Information Systems since 2005. He is the founding director of the Computer Vision and Sensing Systems Laboratory at the University of Windsor, Canada. Prior to joining the university, Dr. Wu was a senior research officer at the National Research Council of Canada. He has published one book in the area of 3D computer vision and more than 350 peer-reviewed papers, including 200 journal articles, in the areas of computer vision, machine learning, sensor data fusion. Dr. Wu is/was an associate editor for IEEE Transactions on Cybenectics, IEEE Transactions on Circuits and Systems for Video Technology, and IEEE Transactions on Neural Networks and Learning Systems. He is an elected fellow of the Canadian Academy of Engineering. </p>
                            <p align="justify" style="padding-right: 5%;"> <strong>Abstract:</strong>Advancements in multisensor data fusion—integrating IR imaging, SAR, hyperspectral imaging, and LiDAR—have significantly enhanced object detection, classification, and scene understanding in applications such as urban monitoring, autonomous navigation, industrial diagnostics, and environmental monitoring. Deep learning-based data fusion, leveraging techniques such as convolutional neural networks (CNNs), transformers, and attention mechanisms, has demonstrated superior performance over traditional approaches in integrating heterogeneous data sources.
                                Despite these advancements, challenges such as data heterogeneity, computational complexity, and real-time processing constraints remain. Recent fusion techniques offer unique advantages in feature integration, decision-making, and robustness against sensor failures. Moreover, novel approaches such as selective sensor fusion, interleaved attention fusion, and multi-scale feature fusion have further enhanced the adaptability and accuracy of deep learning-based fusion models.
                                This keynote will explore state-of-the-art fusion methodologies and generalized inverse-based optimization, highlighting key challenges, case studies, and future research directions. The discussion will focus on scalable and efficient perception systems, addressing how deep learning and graph-based fusion strategies are transforming cybersecurity, intelligent transportation, and healthcare, among other fields. </p>
                        </div>
                    </div>

                    <div class="row">
                        <div class="col-md-4">
                            <img src="images/WilliamFreeman_332x332.jpg" class="center-block img-circle img-responsive"> 
                            <h5 class="text-center">William Freeman </h5> 
                            <p class="text-center">Professor</p>
                            <p class="text-center">Massachusetts Institute of Technology</p>
                        </div>
                        <div class="col-md-8">
                            <h5 class="text-center">Exploiting the assumption of source and sensor independence</h5>
                            <p align="justify" style="padding-right: 5%;"> <strong>Bio:</strong>William T. Freeman is the Thomas and Gerd Perkins Professor of Electrical Engineering and Computer Science (EECS) at MIT, and a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL) there.  He was the Associate Department Head of EECS from 2011 – 2014.  Since 2015, he has also been a research manager in Google Research in Cambridge, MA. His current research interests include mid-level vision and computational photography.  Previous research topics include steerable filters and pyramids, orientation histograms, the generic viewpoint assumption, color constancy, computer vision for computer games, motion magnification, and belief propagation in networks with loops.  He received outstanding paper awards at computer vision or machine learning conferences in 1997, 2006, 2009,  2012 and 2019, and test-of-time awards for papers from 1990, 1995, 2002, 2005, and 2012.    He shared the 2020 Breakthrough Prize in Physics for a consulting role with the Event Horizon Telescope collaboration, which reconstructed the first image of a black hole.  He is a member of the National Academy of Engineering, and a Fellow of the IEEE, ACM, and AAAI.   In 2019, he received the PAMI Distinguished Researcher Award, the highest award in computer vision. </p>
                            <p align="justify" style="padding-right: 5%;"> <strong>Abstract:</strong>  This talk addresses the general problem of source estimation from underconstrained linear inverse problems--estimating x from the observations y = A x, where the measurement matrix A has more columns than rows. Very often, x and A are independent of each other and that fact adds important information that may allow us to lessen our reliance on prior assumptions about x. Exploiting that assumption may be useful in applications in science and medicine.</p>
                        </div>
                    </div>
                </div>
                
                </div>
            </div>
        </div>
    </div>
</div>

-->



</body></html>
