<!DOCTYPE html>
<html>
    <head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <link rel="icon" href="images/logo_o_png.ico"><title>Welcome to PBVS '26</title>
        
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet" href="./css/bootstrap.min.css">
        <script src="./css/jquery.min.js.descarga"></script>
        <script src="./css/bootstrap.min.js.descarga"></script>
        <script src="./js/scrolling-nav.js"></script>
        <link href="./css/font-awesome.min.css" rel="stylesheet" type="text/css">
        <link href="./css/style.css" rel="stylesheet" type="text/css">
        <link href="./css/scrolling-nav.css" rel="stylesheet">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
        <link href="css/modern.css" rel="stylesheet" type="text/css">
    <style>
        #content li {
           margin-left:-20px;
        }
    </style>
    </head>

<body>

<div class="navbar navbar-default navbar-fixed-top" id="navi_bar">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar-ex-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="history.html" style="color:rgb(111,84,153);"><span><strong>22 YEARS of PBVS</strong></span></a>
        </div>
        <div class="collapse navbar-collapse" id="navbar-ex-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li ><a href="index.html"><strong>Home</strong></a></li>
                <li><a href="call_for_paper.html"><strong>Call for Paper</strong></a></li>
                <li><a href="challenge.html"><strong>Challenges</strong></a></li>
                <li><a href="submission.html"><strong>Submission</strong></a></li>
                <li ><a href="organization.html"><strong>Organization</strong></a></li>
                <li><a href="program.html"><strong>Program</strong></a></li>
                <li  class="active"><a href="keynotes.html"><strong>Keynotes</strong></a></li>
                <li ><a href="datasets.html"><strong>Datasets</strong></a></li>
                <li><a href="awards.html"><strong>Awards</strong></a></li>
                <li><a href="history.html"><strong>History</strong></a></li>
            </ul>
        </div>
    </div>
</div>

<div class="section" id="keynotes">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <div class="panel panel-primary">
                    <div class="panel-heading">
                        <h3 class="panel-title text-center">Keynotes</h3>
                    </div>
                    
                    <div class="panel-body">
                    
                        <div class="row">
                        <div class="col-md-12">
                        </div>
                        </div> 
                    </div> 

                    <div class="row">
                        <div class="col-md-4">
                            <img src="images/brian_sheil.jpg" class="center-block img-circle img-responsive"> 
                            <h5 class="text-center">Brian Sheil</h5> 
                            <p class="text-center">Professor</p>
                            <p class="text-center">University of Cambridge, UK</p>
                        </div>
                        <div class="col-md-8">
                            <h5 class="text-center">The Seen and the Unseen: Multi-Modal Sensing for Understanding Surface and Subsurface Infrastructure</h5>
                            <p align="justify" style="padding-right: 5%;"> <strong>Bio:</strong>Brian Sheil is the Director of the Centre for Smart Infrastructure and Construction at University of Cambridge. He previously held academic positions at the University of Oxford and was awarded a Royal Academy of Engineering Research Fellowship before moving to Cambridge in 2022 to take up the Laing O’Rourke Associate Professorship in Construction Engineering. In 2024, he was awarded an EPSRC Open Fellowship for his work on Digital Underground Construction. He is a co-founder and Chief Scientist of the startup InfraMind, which develops AI-based “digital inspectors” for infrastructure, currently being trialled by organisations including National Highways, Network Rail, and Transport for London. He serves on the editorial boards of several leading journals and has received multiple awards for research excellence and commercialisation. His research puts physics first in developing trustworthy AI for infrastructure, combining computer vision, multimodal sensing, physics-informed learning, and multi-scale simulation to enable digital twins, inspection, early warning, and lifecycle management of underground and civil infrastructure. </p>
                            <!--<p align="justify" style="padding-right: 5%;"> <strong>Abstract:</strong>Event cameras are bio-inspired vision sensors with much lower latency, higher dynamic range, and much lower power consumption than standard cameras. This talk will present current trends and opportunities with event cameras, ranging from robotics to virtual reality and smartphones, as well as open challenges and the road ahead.</p> -->
                            <p align="justify" style="padding-right: 5%;"> <strong>Abstract:</strong>TBA</p>
                        </div>
                    </div>
                    <!--
                    <div class="row">
                        <div class="col-md-4">
                            <img src="images/jonathan_wu.png" class="center-block img-circle img-responsive"> 
                            <h5 class="text-center">Jonathan Wu </h5> 
                            <p class="text-center">Professor</p>
                            <p class="text-center">University of Windsor, Canada</p>
                        </div>
                        <div class="col-md-8">
                            <h5 class="text-center">Deep-Learning-Based Multisensor Data Fusion Methods and Applications</h5>
                            <p align="justify" style="padding-right: 5%;"> <strong>Bio:</strong>Dr. Jonathan Wu received a PhD in Computer Vision and Intelligent Systems from the University of Wales, UK. Dr. Wu is a Distinguished professor of electrical and computer engineering and has been a Tier 1 Canada Research Chair in Automotive Sensors and Information Systems since 2005. He is the founding director of the Computer Vision and Sensing Systems Laboratory at the University of Windsor, Canada. Prior to joining the university, Dr. Wu was a senior research officer at the National Research Council of Canada. He has published one book in the area of 3D computer vision and more than 350 peer-reviewed papers, including 200 journal articles, in the areas of computer vision, machine learning, sensor data fusion. Dr. Wu is/was an associate editor for IEEE Transactions on Cybenectics, IEEE Transactions on Circuits and Systems for Video Technology, and IEEE Transactions on Neural Networks and Learning Systems. He is an elected fellow of the Canadian Academy of Engineering. </p>
                            <p align="justify" style="padding-right: 5%;"> <strong>Abstract:</strong>Advancements in multisensor data fusion—integrating IR imaging, SAR, hyperspectral imaging, and LiDAR—have significantly enhanced object detection, classification, and scene understanding in applications such as urban monitoring, autonomous navigation, industrial diagnostics, and environmental monitoring. Deep learning-based data fusion, leveraging techniques such as convolutional neural networks (CNNs), transformers, and attention mechanisms, has demonstrated superior performance over traditional approaches in integrating heterogeneous data sources.
                                Despite these advancements, challenges such as data heterogeneity, computational complexity, and real-time processing constraints remain. Recent fusion techniques offer unique advantages in feature integration, decision-making, and robustness against sensor failures. Moreover, novel approaches such as selective sensor fusion, interleaved attention fusion, and multi-scale feature fusion have further enhanced the adaptability and accuracy of deep learning-based fusion models.
                                This keynote will explore state-of-the-art fusion methodologies and generalized inverse-based optimization, highlighting key challenges, case studies, and future research directions. The discussion will focus on scalable and efficient perception systems, addressing how deep learning and graph-based fusion strategies are transforming cybersecurity, intelligent transportation, and healthcare, among other fields. </p>
                        </div>
                    </div>

                    <div class="row">
                        <div class="col-md-4">
                            <img src="images/WilliamFreeman_332x332.jpg" class="center-block img-circle img-responsive"> 
                            <h5 class="text-center">William Freeman </h5> 
                            <p class="text-center">Professor</p>
                            <p class="text-center">Massachusetts Institute of Technology</p>
                        </div>
                        <div class="col-md-8">
                            <h5 class="text-center">Exploiting the assumption of source and sensor independence</h5>
                            <p align="justify" style="padding-right: 5%;"> <strong>Bio:</strong>William T. Freeman is the Thomas and Gerd Perkins Professor of Electrical Engineering and Computer Science (EECS) at MIT, and a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL) there.  He was the Associate Department Head of EECS from 2011 – 2014.  Since 2015, he has also been a research manager in Google Research in Cambridge, MA. His current research interests include mid-level vision and computational photography.  Previous research topics include steerable filters and pyramids, orientation histograms, the generic viewpoint assumption, color constancy, computer vision for computer games, motion magnification, and belief propagation in networks with loops.  He received outstanding paper awards at computer vision or machine learning conferences in 1997, 2006, 2009,  2012 and 2019, and test-of-time awards for papers from 1990, 1995, 2002, 2005, and 2012.    He shared the 2020 Breakthrough Prize in Physics for a consulting role with the Event Horizon Telescope collaboration, which reconstructed the first image of a black hole.  He is a member of the National Academy of Engineering, and a Fellow of the IEEE, ACM, and AAAI.   In 2019, he received the PAMI Distinguished Researcher Award, the highest award in computer vision. </p>
                            <p align="justify" style="padding-right: 5%;"> <strong>Abstract:</strong>  This talk addresses the general problem of source estimation from underconstrained linear inverse problems--estimating x from the observations y = A x, where the measurement matrix A has more columns than rows. Very often, x and A are independent of each other and that fact adds important information that may allow us to lessen our reliance on prior assumptions about x. Exploiting that assumption may be useful in applications in science and medicine.</p>
                        </div>
                    </div>
                    -->
                </div>
                
                </div>
            </div>
        </div>
    </div>
</div>


</body></html>
