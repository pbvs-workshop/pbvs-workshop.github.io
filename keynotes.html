<!DOCTYPE html>
<html>
    <head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <link rel="icon" href="http://vcipl-okstate.org/pbvs/20/images/logo_o_png.ico"><title>Welcome to PBVS '24</title>
        
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet" href="./css/bootstrap.min.css">
        <script src="./css/jquery.min.js.descarga"></script>
        <script src="./css/bootstrap.min.js.descarga"></script>
        <script src="./js/scrolling-nav.js"></script>
        <link href="./css/font-awesome.min.css" rel="stylesheet" type="text/css">
        <link href="./css/style.css" rel="stylesheet" type="text/css">
        <link href="./css/scrolling-nav.css" rel="stylesheet">
    <style>
        #content li {
           margin-left:-20px;  
        }   
    </style>
    </head>

<body>

<div class="navbar navbar-default navbar-fixed-top" id="navi_bar">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar-ex-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="history.html" style="color:rgb(111,84,153);"><span><strong>20 YEARS of PBVS</strong></span></a>
        </div>
        <div class="collapse navbar-collapse" id="navbar-ex-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li ><a href="index.html"><strong>Home</strong></a></li>
                <li><a href="call_for_paper.html"><strong>Call for Paper</strong></a></li>
                <li><a href="challenge.html"><strong>Challenges</strong></a></li>
                <li><a href="submission.html"><strong>Submission</strong></a></li>
                <li ><a href="organization.html"><strong>Organization</strong></a></li>
                <li><a href="program.html"><strong>Program</strong></a></li>
                <li  class="active"><a href="keynotes.html"><strong>Keynotes</strong></a></li>
                <li ><a href="datasets.html"><strong>Datasets</strong></a></li>
                <li><a href="awards.html"><strong>Awards</strong></a></li>
                <li><a href="history.html"><strong>History</strong></a></li>
            </ul>
        </div>
    </div>
</div>

<div class="section" id="keynotes">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <div class="panel panel-primary">
                    <div class="panel-heading">
                        <h3 class="panel-title text-center">Keynotes</h3>
                    </div>
                    
                    <div class="panel-body">
                    <!--
                        <div class="row">
                        <div class="col-md-12">
                            <p align="center"><strong>Coming soon ... </strong></p>
                        </div>
                        </div> 
                    </div> 
                    -->

                    <div class="row">
                        <div class="col-md-4">
                            <img src="images/avatar_200.jpg" class="center-block img-circle img-responsive"> 
                            <h5 class="text-center">Riad Hammoud</h5> 
                            <p class="text-center">Director</p>
                            <p class="text-center">PlusAI, USA</p>
                        </div>
                        <div class="col-md-8">
                            <h5 class="text-center">Exploring the potentials of EM spectrum: 20 Years of Perception Beyond the Visible Spectrum at CVPR</h5>
                            <p align="justify"><strong>Bio:</strong>Coming soon ... </p>
                            <p align="justify"><strong>Abstract:</strong>Coming soon ... </p>
                        </div>
                    </div>

                    <div class="row">
                        <div class="col-md-4">
                            <img src="images/Dr_Shah.jpg" class="center-block img-circle img-responsive"> 
                            <h5 class="text-center">Mubarak Shah</h5> 
                            <p class="text-center">Professor</p>
                            <p class="text-center">University of Central Florida, USA</p>
                        </div>
                        <div class="col-md-8">
                            <h5 class="text-center">Beyond Visible Spectrum: Twenty Year Retrospective</h5>
                            <p align="justify"><strong>Bio:</strong>Coming soon ... </p>
                            <p align="justify"><strong>Abstract:</strong>Coming soon ... </p>
                        </div>
                    </div>


                       <div class="row">
                            <div class="col-md-4">
                                <img src="images/ommer_bjoern_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Björn Ommer</h5> 
                                <p class="text-center">Professor</p>
                                <p class="text-center">University of Munich, Germany</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Boosting Diffusion Models for Visual Synthesis</h5>
                                <p align="justify"><strong>Bio:</strong>Björn Ommer is a full professor at Ludwig Maximilian University of Munich where he is heading the Computer Vision & Learning Group. Before, he was a full professor in the department of mathematics and computer science at Heidelberg University and a co-director of its Interdisciplinary Center for Scientific Computing. He received his diploma in computer science from University of Bonn, his PhD from ETH Zurich, and he was a postdoc at UC Berkeley. Björn serves in the Bavarian AI council and has been an associate editor for IEEE T-PAMI. His research interests include semantic scene understanding and retrieval, generative AI and visual synthesis, self-supervised metric and representation learning, and explainable AI. Moreover, he is applying this basic research in interdisciplinary projects within neuroscience and the digital humanities. His group has published a series of generative approaches, including work known as "VQGAN" and "Stable Diffusion", which are now democratizing the creation of visual content and have already opened up an abundance of new directions in research, industry, the media, and beyond.</p>
                                <p align="justify"><strong>Abstract:</strong> Coming soon... </p>
                            </div>
                        </div>
                          
                    <!--     
                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/rikke_gade_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Rikke Gade</h5> 
                                <p class="text-center">Associate Professor</p>
                                <p class="text-center">Aalborg University, Denmark</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Beyond Visible Spectrum: Twenty Year Retrospective</h5>
                                <p align="justify"><strong>Bio:</strong>Rikke Gade is currently employed as Associate Professor at Aalborg University, Denmark, in the Visual Analysis of People Lab. She received her M.Sc. and PhD degrees from Aalborg University in 2011 and 2015, respectively. During her studies she has also visited University of Auckland, New Zealand and University of Adelaide, Australia. The PhD thesis in Computer Vision focused on analysis of activities in sports arenas; mainly occupancy analysis, activity recognition, and tracking of players. Most of her work revolves around the use of thermal video, to preserve privacy in public sports facilities. This has led to publications in top journals and international conferences within computer vision, and she has been co-organizing workshops at CVPR, ICCV, and ACCV. Her research interests include computer vision analysis of human activities, applied both for analysis of human behaviour at public spaces as well as for analysing sports activities. She also works with robot vision.</p>
                                <p align="justify"><strong>Abstract:</strong>In this talk I will dive into our story of using thermal cameras for privacy preserving computer vision algorithms at Visual Analysis and Perception lab at Aalborg University. My first encounter with thermal imaging was back in 2011 when thermal cameras were rarely seen in public computer vision research. As part of my PhD project I captured and analyzed long-term thermal datasets of a variety of human activities in both indoor and outdoor environments. The majority of our work revolves around sports applications such as occupancy analysis of sports arenas (indoor and outdoor) and analysis of sports activities where thermal cameras are used instead of regular RGB cameras to preserve privacy on public facilities. The computer vision methods applied ranges from low-level image processing and machine learning to deep learning on our more recent work. Other applications I will cover in this talk include clothing level estimation in office environments and detection of accidents in open harbour areas. During my talk I will, among other things, share insights on what to pay special attention to when using thermal cameras and demonstrate how methods designed for RGB images can be adapted to the thermal domain.</p>
                            </div>
                        </div>
                        
                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/Robert_Laganiere_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Robert Laganiere</h5> 
                                <p class="text-center">Professor</p>
                                <p class="text-center">University of Ottawa, Canada</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Perception from Radar sensors: principles and challenges</h5>
                                <p align="justify"><strong>Bio:</strong>Robert is a professor at the School of Electrical Engineering and Computer Science of the University of Ottawa and the CEO of Sensor Cortek, a startup company developing AI solutions for perception systems. Robert is the co-author of several scientific publications and patents in content-based video analysis, visual surveillance, embedded vision, driver-assistance and autonomous driving applications. Robert authored the OpenCV2 Computer Vision Application Programming Cookbook (2011) and co-authored Object Oriented Software Development (2001). He co-founded Visual Cortek in 2006, an Ottawa-based video analytics startup that was later acquired by iWatchLife in 2009. He also co-founded Tempo Analytics in 2016 a company proposing Retail Analytics solutions and founded Sensor Cortek inc in 2018.
                                </p>
                                <p align="justify"><strong>Abstract:</strong> Radar is one of the essential technologies that enables machines to perceive and interact with the world around them. By sensing the environment using radio waves, radar provides information about a scene and its objects that can be used in a wide range of applications, from autonomous driving to surveillance and security. Radar is an old technology that continues to evolve and that is expected to play an important role in the perception systems of the future. In this presentation, a survey of the radar technology and its benefit will be provided. We will explain how radar can extract range, azimuth and velocity information to detect objects and their speed. We will also explore some of the recent development that uses AI to improve radar perception and discuss the particular challenges that pose radar data in the context of deep learning.</p>
                            </div>
                        </div>
                        
                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/Zheng_Liu_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Zheng Liu</h5> 
                                <p class="text-center">Professor</p>
                                <p class="text-center">The University of British Columbia, Canada</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Towards a Comprehensive Perception: Methodologies for Thermal Imaging Data Analysis</h5>
                                <p align="justify"><strong>Bio:</strong> Zheng Liu is a professor at the Faculty of Applied Science of the University of British Columbia (UBC Okanagan). Before joining UBC, he worked for the National Research Council of Canada as a research officer and for the Toyota Technological Institute (Nagoya) as a professor. His research interests include digital twin, data/information fusion, computer/machine vision, machine learning, smart sensor and industrial IoT, and non-destructive inspection and evaluation. Dr. Liu is a fellow of SPIE and a senior member of IEEE and holds Professional Engineer licenses in both British Columbia and Ontario. In addition, Dr. Liu serves on the editorial boards for journals including Information Fusion (Elsevier), Machine Vision and Applications (Springer), IEEE Transactions on Instrumentation and Measurement, IEEE Transactions on AgriFood Electronics, and IEEE Journal of RFID, and CAAI Transactions on intelligence technology.</p>
                                <p align="justify"><strong>Abstract:</strong> Many industrial sectors and applications have benefited from thermal imaging technology for its perception capability enabled by its spectrum and advances in computational methodologies in varied situations. A thermal imaging system can be configured or operated in different modes, e.g., unimodal and multimodal, with or without explicit fusion operations. However, what thermal imaging contributes to human perception depends on the forms of the derived information, i.e., how the thermal imaging data are processed with the auxiliary data and information. This talk will overview the methodologies for processing and analyzing thermal imaging data in the context of application needs. The research opportunities and challenges will be highlighted in the presentation.</p>
                                </div>
                        </div>

                
                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/Kilian_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Prof. Kilian Weinberger</h5> 
                                <p class="text-center">Associate Professor</p>
                                <p class="text-center">Cornell University, USA</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Deep Learning with Depth Perception - Representation Matters </h5>
                                <p align="justify"><strong>Bio:</strong> Kilian Weinberger is an Associate Professor in the Department of Computer Science at Cornell University. He received his Ph.D. from the University of Pennsylvania in Machine Learning and his undergraduate degree in Mathematics and Computing from the University of Oxford. During his career he has won several best paper awards, at ICML (2004), CVPR (2004, 2017), AISTATS (2005) and KDD (2014, runner-up award). In 2011 he was awarded the Outstanding AAAI Senior Program Chair Award and in 2012 he received an NSF CAREER award. He was elected co-Program Chair for ICML 2016 and for AAAI 2018. Currently he serves as an elected board member for the international machine learning society. In 2016 he was the recipient of the Daniel M Lazar '29 Excellence in Teaching Award. Kilian Weinberger's research focuses on Machine Learning and its applications, in particular on learning under resource constraints, metric learning, Gaussian Processes, computer vision and deep learning. Before joining Cornell University, he was an Associate Professor at Washington University in St. Louis and before that he worked as a research scientist at Yahoo! Research in Santa Clara. </p>
                                <p align="justify"><strong>Abstract:</strong> 3D object detection is an essential task in autonomous driving. Recent techniques excel with highly accurate detection rates, provided the 3D input data is obtained from precise but expensive LiDAR technology. Approaches based on cheaper monocular or stereo imagery data have, until recently, resulted in drastically lower accuracies - a gap that was commonly attributed to poor image-based depth estimation. In this talk we argue that it is not the precision or quality of the data, but its representation that accounts for the majority of the difference. Taking the inner workings of convolutional neural networks into consideration, we propose to convert image-based depth maps to pseudo-LiDAR representations - essentially mimicking the LiDAR signal. With this representation we can apply different existing LiDAR-based detection algorithms. Our approaches yield impressive improvements on the popular KITTI 3D object detection benchmark data set, and have already been adapted by the self-driving car community.</p>
                            </div>
                        </div>

                        <div class="row">
                            <div class="col-md-4">
                                <img src="images/Volkan_200.jpg" class="center-block img-circle img-responsive"> 
                                <h5 class="text-center">Prof. Volkan Cevher</h5> 
                                <p class="text-center">Associate Professor</p>
                                <p class="text-center">Swiss Federal Institute of Technology in Lausanne, Switzerland</p>
                            </div>
                            <div class="col-md-8">
                                <h5 class="text-center">Adversarial machine learning: Recent developments</h5>
                                <p align="justify"><strong>Bio:</strong> Volkan Cevher received the B.Sc. (valedictorian) in electrical engineering from Bilkent University in Ankara, Turkey, in 1999 and the Ph.D. in electrical and computer engineering from the Georgia Institute of Technology in Atlanta, GA in 2005. He was a Research Scientist with the University of Maryland, College Park from 2006-2007 and also with Rice University in Houston, TX, from 2008-2009. Currently, he is an Associate Professor at the Swiss Federal Institute of Technology Lausanne and a Faculty Fellow in the Electrical and Computer Engineering Department at Rice University. His research interests include machine learning, signal processing theory, optimization, and information theory. Dr. Cevher is an ELLIS fellow and was the recipient of the Google Faculty Research Award on Machine Learning in 2018, IEEE Signal Processing Society Best Paper Award in 2016, a Best Paper Award at CAMSAP in 2015, a Best Paper Award at SPARS in 2009, and an ERC CG in 2016 as well as an ERC StG in 2011.</p>
                                <p align="justify"><strong>Abstract:</strong> Thanks to neural networks (NNs), faster computation, and massive datasets, machine learning (ML) is under increasing pressure to provide automated solutions to even harder real-world tasks beyond human performance with ever faster response times due to potentially huge technological and societal benefits. Unsurprisingly, the NN learning formulations present a fundamental challenge to the back-end learning algorithms despite their scalability, in particular due to the existence traps in the non-convex optimization landscape, such as saddle points, that can prevent algorithms to obtain “good” solutions.<br><br> In this talk, we describe our recent research that has demonstrated that the non-convex optimization dogma is false by showing that scalable stochastic optimization algorithms can avoid traps and rapidly obtain locally optimal solutions. Coupled with the progress in representation learning, such as over-parameterized neural networks, such local solutions can be globally optimal. <br><br>Unfortunately, this talk will also demonstrate that the central min-max optimization problems in ML, such as generative adversarial networks (GANs), robust reinforcement learning (RL), and distributionally robust ML, contain spurious attractors that do not include any stationary points of the original learning formulation. Indeed, we will describe how algorithms are subject to a grander challenge, including unavoidable convergence failures, which could explain the stagnation in their progress despite the impressive earlier demonstrations.</p>
                            </div>
                        </div>
                    -->
                </div>
                
                </div>
            </div>
        </div>
    </div>
</div>





</body></html>